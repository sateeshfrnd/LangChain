{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion using Documentloaders\n",
    "\n",
    "A Document Loader in LangChain is a tool that helps load data from various sources, such as text files, PDFs, web pages, databases, and more. Once the data is loaded, it can be used for natural language processing (NLP), question answering, summarization, and chatbots.\n",
    "\n",
    "- *API* : https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x21623e1c5e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "textloader = TextLoader(\"sample.txt\")\n",
    "textloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content=\"LangChain is an open-source framework designed to help developers build applications powered by large language models (LLMs).  \\n\\nIt provides tools for loading, processing, and managing different types of data sources such as text files, PDFs, web pages, and databases.  \\n\\nUsing LangChain's document loaders, we can efficiently fetch data from multiple sources and utilize it for various AI-based tasks.\\n\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_doc = textloader.load()\n",
    "text_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it will read the content of sample.txt as one document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Attention_is_ All_You_Need.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "pdf_loader = PyPDFLoader(\"Attention_is_ All_You_Need.pdf\")\n",
    "pdf_doc = pdf_loader.load()\n",
    "pdf_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the content of the PDF and store each page as a separate document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Web Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x21649c4a0b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "webloader = WebBaseLoader(web_path=(\"https://medium.com/@sateeshfrnd/understanding-the-langchain-ecosystem-616b33f5cd15\"))\n",
    "webloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x2164a28f2b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webloader = WebBaseLoader(web_paths=(\"https://github.com/sateeshfrnd/sateeshfrnd/blob/main/README.md\", ),)\n",
    "webloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://github.com/sateeshfrnd/sateeshfrnd/blob/main/README.md', 'title': 'sateeshfrnd/README.md at main · sateeshfrnd/sateeshfrnd · GitHub', 'description': 'Config files for my GitHub profile. Contribute to sateeshfrnd/sateeshfrnd development by creating an account on GitHub.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nsateeshfrnd/README.md at main · sateeshfrnd/sateeshfrnd · GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\nCodespaces\\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\nIssues\\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode Review\\n        Manage code changes\\n      \\n\\n\\n\\n\\n\\n\\n\\nDiscussions\\n        Collaborate outside of code\\n      \\n\\n\\n\\n\\n\\n\\n\\nCode Search\\n        Find more, search less\\n      \\n\\n\\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      All features\\n\\n    \\n\\n\\n\\n      Documentation\\n\\n    \\n\\n\\n\\n\\n\\n      GitHub Skills\\n\\n    \\n\\n\\n\\n\\n\\n      Blog\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Solutions\\n        \\n\\n\\n\\n\\n\\n\\nBy company size\\n\\n\\n\\n      Enterprises\\n\\n    \\n\\n\\n\\n      Small and medium teams\\n\\n    \\n\\n\\n\\n      Startups\\n\\n    \\n\\n\\n\\n      Nonprofits\\n\\n    \\n\\n\\n\\n\\nBy use case\\n\\n\\n\\n      DevSecOps\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      CI/CD\\n\\n    \\n\\n\\n\\n      View all use cases\\n\\n    \\n\\n\\n\\n\\n\\n\\nBy industry\\n\\n\\n\\n      Healthcare\\n\\n    \\n\\n\\n\\n      Financial services\\n\\n    \\n\\n\\n\\n      Manufacturing\\n\\n    \\n\\n\\n\\n      Government\\n\\n    \\n\\n\\n\\n      View all industries\\n\\n    \\n\\n\\n\\n\\n\\n\\n              View all solutions\\n              \\n\\n\\n \\n\\n\\n\\n\\n        Resources\\n        \\n\\n\\n\\n\\n\\n\\nTopics\\n\\n\\n\\n      AI\\n\\n    \\n\\n\\n\\n      DevOps\\n\\n    \\n\\n\\n\\n      Security\\n\\n    \\n\\n\\n\\n      Software Development\\n\\n    \\n\\n\\n\\n      View all\\n\\n    \\n\\n\\n\\n\\n\\n\\nExplore\\n\\n\\n\\n      Learning Pathways\\n\\n    \\n\\n\\n\\n\\n\\n      White papers, Ebooks, Webinars\\n\\n    \\n\\n\\n\\n\\n\\n      Customer Stories\\n\\n    \\n\\n\\n\\n      Partners\\n\\n    \\n\\n\\n\\n\\n\\n      Executive Insights\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n        Open Source\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Sponsors\\n        Fund open source developers\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nThe ReadME Project\\n        GitHub community articles\\n      \\n\\n\\n\\n\\nRepositories\\n\\n\\n\\n      Topics\\n\\n    \\n\\n\\n\\n      Trending\\n\\n    \\n\\n\\n\\n      Collections\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n        Enterprise\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnterprise platform\\n        AI-powered developer platform\\n      \\n\\n\\n\\n\\nAvailable add-ons\\n\\n\\n\\n\\n\\n\\n\\nAdvanced Security\\n        Enterprise-grade security features\\n      \\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Enterprise-grade AI features\\n      \\n\\n\\n\\n\\n\\n\\n\\nPremium Support\\n        Enterprise-grade 24/7 support\\n      \\n\\n\\n\\n\\n\\n\\n\\nPricing\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch or jump to...\\n\\n\\n\\n\\n\\n\\n\\nSearch code, repositories, users, issues, pull requests...\\n\\n \\n\\n\\n\\n\\n        Search\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClear\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nSearch syntax tips \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Provide feedback\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nWe read every piece of feedback, and take your input very seriously.\\n\\n\\nInclude my email address so I can be contacted\\n\\n\\n     Cancel\\n\\n    Submit feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Saved searches\\n      \\nUse saved searches to filter your results more quickly\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nName\\n\\n\\n\\n\\n\\n\\nQuery\\n\\n\\n\\n            To see all available qualifiers, see our documentation.\\n          \\n \\n\\n\\n\\n\\n\\n     Cancel\\n\\n    Create saved search\\n\\n\\n\\n\\n\\n\\n\\n\\n                Sign in\\n              \\n\\n\\n                Sign up\\n              \\nReseting focus\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.\\n \\n\\n\\nDismiss alert\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        sateeshfrnd\\n \\n/\\n\\nsateeshfrnd\\n\\nPublic\\n\\n\\n\\n\\n\\n \\n\\nNotifications\\n You must be signed in to change notification settings\\n\\n\\n \\n\\nFork\\n    0\\n\\n\\n\\n\\n \\n\\n\\n          Star\\n 1\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCode\\n\\n\\n\\n\\n\\n\\n\\nPull requests\\n0\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\n\\n\\n\\n\\n\\nProjects\\n0\\n\\n\\n\\n\\n\\n\\nSecurity\\n\\n\\n\\n\\n\\n\\n\\nInsights\\n\\n\\n\\n \\n\\n \\n\\n\\nAdditional navigation options\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Code\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Pull requests\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Actions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Projects\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Security\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Insights\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n   Files\\xa0mainBreadcrumbssateeshfrnd/README.mdCopy path Blame  Blame        Latest commit\\xa0HistoryHistory49 lines (36 loc) · 2.54 KB\\xa0mainBreadcrumbssateeshfrnd/README.mdTopFile metadata and controlsPreviewCodeBlame49 lines (36 loc) · 2.54 KBRawHello, I am Satish 👋 \\nA passionate coder and perpetual learner\\n\\n🔹 About Me\\nHi, I'm Satish Kumar, a Data Engineer. I specialize in building scalable end-to-end data pipelines and working with big data technologies to drive impactful insights. Passionate about Data Engineering, Machine Learning, and Generative AI, I enjoy solving complex data problems and optimizing data workflows.\\n🚀 Currently Exploring:\\n\\nBuilding real-world Data Engineering systems using Kafka, Spark, Airflow, Postgres, and Docker.\\nNLP and Generative AI, focusing on Large Language Models.\\nData as a Product and Data Governance best practices.\\nHow to architectect better data system\\n\\n✍️ Blog Posts & Learning\\n\\nMy Medium Blog\\nLinkedIn Articles\\nMy Wordpress\\n\\n📚 Skills & Tech Stack\\n\\nProgramming Languages: HTML/CSS/JavaScript, Java, Python, SQL, Scala\\nDatabases: Oracle DB, SQL Server, MySQL, PostgreSQL\\nBig Data & Cloud: Hadoop, YARN, MapReduce, Sqoop, Flume, Hive, HBase, Zookeeper, Spark, Cloudera Tech stacks, AWS, Databricks, Snowflake\\nStreaming & Orchestration: Kafka, Oozie, AirFlow,\\nNLP & AI: Transformers, LLMs, TextBlob, OpenAI APIs\\nVisualization: Matplotlib, Seaborn, Streamlit\\nCloud/DevOps: Git, Jenkins, Docker\\nTools: Jira, Confluence\\n\\n📌 My Certifications and Courses\\n\\nPython for Everybody Specialization\\nAWS Certified Solutions Architect – Associate\\nProgramming for Data Science\\nCloud DevOps Engineer\\n\\n📬 Connect With Me\\n\\n💼 LinkedIn: Satish Kumar\\n✉️ Email: sateeshfrnd@gmail.com\\n\\nFeel free to explore my repositories, share feedback, or collaborate on projects. Let's build something incredible together!\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\n        © 2025 GitHub,\\xa0Inc.\\n      \\n\\n\\nFooter navigation\\n\\n\\nTerms\\n\\n\\nPrivacy\\n\\n\\nSecurity\\n\\n\\nStatus\\n\\n\\nDocs\\n\\n\\nContact\\n\\n\\n\\n\\n      Manage cookies\\n    \\n\\n\\n\\n\\n\\n      Do not share my personal information\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    You can’t perform that action at this time.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webloader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x21649c43a30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "webloader1 = WebBaseLoader(web_paths=(\"https://github.com/sateeshfrnd/sateeshfrnd/blob/main/README.md\", ),\n",
    "                           bs_kwargs=dict(\n",
    "                               parse_only = bs4.SoupStrainer(class_ =(\"markdown-body entry-content container-lg\",\"markdown-heading\",))\n",
    "                            )\n",
    "                           )\n",
    "webloader1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://github.com/sateeshfrnd/sateeshfrnd/blob/main/README.md'}, page_content=\"Hello, I am Satish 👋 \\nA passionate coder and perpetual learner\\n\\n🔹 About Me\\nHi, I'm Satish Kumar, a Data Engineer. I specialize in building scalable end-to-end data pipelines and working with big data technologies to drive impactful insights. Passionate about Data Engineering, Machine Learning, and Generative AI, I enjoy solving complex data problems and optimizing data workflows.\\n🚀 Currently Exploring:\\n\\nBuilding real-world Data Engineering systems using Kafka, Spark, Airflow, Postgres, and Docker.\\nNLP and Generative AI, focusing on Large Language Models.\\nData as a Product and Data Governance best practices.\\nHow to architectect better data system\\n\\n✍️ Blog Posts & Learning\\n\\nMy Medium Blog\\nLinkedIn Articles\\nMy Wordpress\\n\\n📚 Skills & Tech Stack\\n\\nProgramming Languages: HTML/CSS/JavaScript, Java, Python, SQL, Scala\\nDatabases: Oracle DB, SQL Server, MySQL, PostgreSQL\\nBig Data & Cloud: Hadoop, YARN, MapReduce, Sqoop, Flume, Hive, HBase, Zookeeper, Spark, Cloudera Tech stacks, AWS, Databricks, Snowflake\\nStreaming & Orchestration: Kafka, Oozie, AirFlow,\\nNLP & AI: Transformers, LLMs, TextBlob, OpenAI APIs\\nVisualization: Matplotlib, Seaborn, Streamlit\\nCloud/DevOps: Git, Jenkins, Docker\\nTools: Jira, Confluence\\n\\n📌 My Certifications and Courses\\n\\nPython for Everybody Specialization\\nAWS Certified Solutions Architect – Associate\\nProgramming for Data Science\\nCloud DevOps Engineer\\n\\n📬 Connect With Me\\n\\n💼 LinkedIn: Satish Kumar\\n✉️ Email: sateeshfrnd@gmail.com\\n\\nFeel free to explore my repositories, share feedback, or collaborate on projects. Let's build something incredible together!\\n\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webloader1.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only reading particular sections only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.arxiv.ArxivLoader at 0x2164a28f520>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "arxicloader = ArxivLoader(query=\"1706.03762\") # Load a research paper\n",
    "arxicloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxicloader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Artificial intelligence', 'summary': 'Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence'}, page_content='Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field\\'s long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\n\\n\\n== Goals ==\\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\n\\n=== Reasoning and problem-solving ===\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\\n\\n\\n=== Knowledge representation ===\\n\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used '),\n",
       " Document(metadata={'title': 'Artificial general intelligence', 'summary': 'Artificial general intelligence (AGI) is a type of artificial intelligence (AI) that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks. Artificial superintelligence (ASI), on the other hand, refers to AGI that greatly exceeds human cognitive capabilities. AGI is considered one of the definitions of strong AI.\\nCreating AGI is a primary goal of AI research and of companies such as OpenAI and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\\nThe timeline for achieving AGI remains a subject of ongoing debate among researchers and experts. As of 2023, some argue that it may be possible in years or decades; others maintain it might take a century or longer; a minority believe it may never be achieved; and another minority claims that it is already here. Notable AI researcher Geoffrey Hinton has expressed concerns about the rapid progress towards AGI, suggesting it could be achieved sooner than many expect.\\nThere is debate on the exact definition of AGI and regarding whether modern large language models (LLMs) such as GPT-4 are early forms of AGI. AGI is a common topic in science fiction and futures studies.\\nContention exists over whether AGI represents an existential risk. Many experts on AI have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be too remote to present such a risk.', 'source': 'https://en.wikipedia.org/wiki/Artificial_general_intelligence'}, page_content='Artificial general intelligence (AGI) is a type of artificial intelligence (AI) that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks. Artificial superintelligence (ASI), on the other hand, refers to AGI that greatly exceeds human cognitive capabilities. AGI is considered one of the definitions of strong AI.\\nCreating AGI is a primary goal of AI research and of companies such as OpenAI and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\\nThe timeline for achieving AGI remains a subject of ongoing debate among researchers and experts. As of 2023, some argue that it may be possible in years or decades; others maintain it might take a century or longer; a minority believe it may never be achieved; and another minority claims that it is already here. Notable AI researcher Geoffrey Hinton has expressed concerns about the rapid progress towards AGI, suggesting it could be achieved sooner than many expect.\\nThere is debate on the exact definition of AGI and regarding whether modern large language models (LLMs) such as GPT-4 are early forms of AGI. AGI is a common topic in science fiction and futures studies.\\nContention exists over whether AGI represents an existential risk. Many experts on AI have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be too remote to present such a risk.\\n\\n\\n== Terminology ==\\nAGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action.\\nSome academic sources reserve the term \"strong AI\" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.\\nRelated concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.\\nA framework for classifying AGI in levels was proposed in 2023 by Google DeepMind researchers. They define five levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI.\\n\\n\\n== Characteristics ==\\n\\nVarious popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches. \\n\\n\\n=== Intelligence traits ===\\nResearchers generally hold that intelligence is required to do all of the following:\\n\\nreason, use strategy, solve puzzles, and make judgments under uncertainty\\nrepresent knowledge, including common sense knowledge\\nplan\\nlearn\\ncommunicate in natural language\\nif necessary, integrate these skills in completion of any given goal\\nMany interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.\\nComputer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI s'),\n",
       " Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nSince its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automations has flourished throughout history, exemplified by Maillardet\\'s automaton created in the early 1800s. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.\\n\\n\\n=== Academic artificial intelligence ===\\nThe academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural nets (2014-2019) ===\\n\\nSince its inception, the field of machine learning used both discriminative models and generative models, to model and '),\n",
       " Document(metadata={'title': 'A.I. Artificial Intelligence', 'summary': 'A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O\\'Connor, Brendan Gleeson and William Hurt star in supporting roles.\\nDevelopment of A.I. originally began after producer and director Stanley Kubrick acquired the rights to Aldiss\\'s story in the early 1970s. Kubrick hired a series of writers, including Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick died in 1999. Spielberg remained close to Watson\\'s treatment for the screenplay and dedicated the film to Kubrick.\\nA.I. Artificial Intelligence was released on June 29, 2001, by Warner Bros. Pictures in North America. It received generally positive reviews from critics and grossed $235.9 million against a budget of $90–100 million. It was also nominated for Best Visual Effects and Best Original Score (for John Williams) at the 74th Academy Awards. In a 2016 BBC poll of 177 critics around the world, A.I. Artificial Intelligence was voted the eighty-third greatest film since 2000. It has since been called one of Spielberg\\'s best works and one of the greatest films of the 21st century, and of all time.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/A.I._Artificial_Intelligence'}, page_content='A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O\\'Connor, Brendan Gleeson and William Hurt star in supporting roles.\\nDevelopment of A.I. originally began after producer and director Stanley Kubrick acquired the rights to Aldiss\\'s story in the early 1970s. Kubrick hired a series of writers, including Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick died in 1999. Spielberg remained close to Watson\\'s treatment for the screenplay and dedicated the film to Kubrick.\\nA.I. Artificial Intelligence was released on June 29, 2001, by Warner Bros. Pictures in North America. It received generally positive reviews from critics and grossed $235.9 million against a budget of $90–100 million. It was also nominated for Best Visual Effects and Best Original Score (for John Williams) at the 74th Academy Awards. In a 2016 BBC poll of 177 critics around the world, A.I. Artificial Intelligence was voted the eighty-third greatest film since 2000. It has since been called one of Spielberg\\'s best works and one of the greatest films of the 21st century, and of all time.\\n\\n\\n== Plot ==\\nIn the 22nd century, rising sea levels from global warming have wiped out coastal cities and altered the world\\'s climate. With the human population in decline, advanced nations have created Mecha to fulfill various roles in society.\\nIn Madison, New Jersey, David, an 11-year-old prototype Mecha child capable of experiencing love, is given to Henry Swinton and his wife Monica, whose son Martin is in suspended animation after contracting a rare disease. Initially uncomfortable with David, Monica eventually warms to him and activates his imprinting protocol. Wanting her to love him in return, he befriends Teddy, Martin\\'s old robotic teddy bear.\\nAfter Martin is unexpectedly cured of his disease and brought home, he jealously goads David into cutting off a piece of Monica\\'s hair. That night, David enters his adoptive parents\\' room, but as Monica turns over, the scissors accidentally poke her in the eye. While Henry attends to her wounds, Teddy picks up the lock of hair from the floor and places it in his pocket. During a pool party, one of Martin\\'s friends pokes David with a knife, triggering his self-protection programming. David grabs Martin, causing both of them to fall into the pool. While Martin is rescued, David is accused of endangering others.\\nHenry convinces Monica to return David to his creators for destruction. En route, she instead spares David by abandoning him in the woods full of scrap metal and obsolete Mecha. Now accompanied solely by Teddy, David recalls The Adventures of Pinocchio and decides to find the Blue Fairy to become human, which he believes will regain Monica\\'s love.\\nDavid and Teddy are captured by the \"Flesh Fair\", a traveling circus-like event at which obsolete Mecha are destroyed in front of jeering crowds. About to be destroyed himself, David pleads for his life, and the audience revolts and allows David to escape with Gigolo Joe, a prostitute Mecha on the run after being framed for murder. David, Teddy and Joe go to the decadent resort town of Rouge City, where \"Dr. Know\", a holographic answer engine, directs them to the top of Rockefeller Center in the flooded ruins of New York City and provides fairy tale information that David interprets as sugge'),\n",
       " Document(metadata={'title': 'History of artificial intelligence', 'summary': 'The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true.\\nEventually, it became obvious that researchers had grossly underestimated the difficulty of this feat. In 1974, criticism from James Lighthill and pressure from the U.S. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors\\' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names.\\nIn the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.\\nInvestment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, prompting debate about the future of AI and its impact on society.', 'source': 'https://en.wikipedia.org/wiki/History_of_artificial_intelligence'}, page_content='The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true.\\nEventually, it became obvious that researchers had grossly underestimated the difficulty of this feat. In 1974, criticism from James Lighthill and pressure from the U.S. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors\\' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names.\\nIn the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.\\nInvestment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, prompting debate about the future of AI and its impact on society.\\n\\n\\n== Precursors ==\\n\\n\\n=== Mythical, fictional, and speculative precursors ===\\n\\n\\n==== Myth and legend ====\\nIn Greek mythology, Talos was a giant made of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island\\'s perimeter daily. According to pseudo-Apollodorus\\' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless.\\nPygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid\\'s Metamorphoses. In the 10th book of Ovid\\'s narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.\\n\\n\\n==== Medieval legends of artificial beings ====\\n\\nIn Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an \"artificial man\". By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant.\\nThe earlie'),\n",
       " Document(metadata={'title': 'Artificial intelligence industry in China', 'summary': 'The artificial intelligence industry in the People\\'s Republic of China is a rapidly developing multi-billion dollar industry. The roots of China\\'s AI development started in the late 1970s following Deng Xiaoping\\'s economic reforms emphasizing science and technology as the country\\'s primary productive force.\\nThe initial stages of China\\'s AI development were slow and encountered significant challenges due to  lack of resources and talent. At the beginning China was behind most Western countries in terms of AI development. A majority of the research was led by scientists who had received higher education abroad.\\nSince 2006, the government of the People\\'s Republic of China has steadily developed a national agenda for artificial intelligence development and emerged as one of the leading nations in artificial intelligence research and development. In 2016, the Chinese Communist Party (CCP) released its thirteenth five-year plan in which it aimed to become a global AI leader by 2030.\\nThe State Council has a list of \"national AI teams\" including fifteen China-based companies, including Baidu, Tencent, Alibaba, SenseTime, and iFlytek. Each company should lead the development of a designated specialized AI sector in China, such as facial recognition, software/hardware, and speech recognition. China\\'s rapid AI development has significantly impacted Chinese society in many areas, including the socio-economic, military, and political spheres. Agriculture, transportation, accommodation and food services, and manufacturing are the top industries that would be the most impacted by further AI deployment.\\nThe private sector, university laboratories, and the military are working collaboratively in many aspects as there are few current existing boundaries. In 2021, China published the Data Security Law of the People\\'s Republic of China, its first national law addressing AI-related ethical concerns. In October 2022, the United States federal government announced a series of export controls and trade restrictions intended to restrict China\\'s access to advanced computer chips for AI applications.\\nConcerns have been raised about the effects of the Chinese government\\'s censorship regime on the development of generative artificial intelligence and talent acquisition with state of the country\\'s demographics.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence_industry_in_China'}, page_content='The artificial intelligence industry in the People\\'s Republic of China is a rapidly developing multi-billion dollar industry. The roots of China\\'s AI development started in the late 1970s following Deng Xiaoping\\'s economic reforms emphasizing science and technology as the country\\'s primary productive force.\\nThe initial stages of China\\'s AI development were slow and encountered significant challenges due to  lack of resources and talent. At the beginning China was behind most Western countries in terms of AI development. A majority of the research was led by scientists who had received higher education abroad.\\nSince 2006, the government of the People\\'s Republic of China has steadily developed a national agenda for artificial intelligence development and emerged as one of the leading nations in artificial intelligence research and development. In 2016, the Chinese Communist Party (CCP) released its thirteenth five-year plan in which it aimed to become a global AI leader by 2030.\\nThe State Council has a list of \"national AI teams\" including fifteen China-based companies, including Baidu, Tencent, Alibaba, SenseTime, and iFlytek. Each company should lead the development of a designated specialized AI sector in China, such as facial recognition, software/hardware, and speech recognition. China\\'s rapid AI development has significantly impacted Chinese society in many areas, including the socio-economic, military, and political spheres. Agriculture, transportation, accommodation and food services, and manufacturing are the top industries that would be the most impacted by further AI deployment.\\nThe private sector, university laboratories, and the military are working collaboratively in many aspects as there are few current existing boundaries. In 2021, China published the Data Security Law of the People\\'s Republic of China, its first national law addressing AI-related ethical concerns. In October 2022, the United States federal government announced a series of export controls and trade restrictions intended to restrict China\\'s access to advanced computer chips for AI applications.\\nConcerns have been raised about the effects of the Chinese government\\'s censorship regime on the development of generative artificial intelligence and talent acquisition with state of the country\\'s demographics.\\n\\n\\n== History ==\\nThe research and development of artificial intelligence in China started in the 1980s, with the announcement by Deng Xiaoping of the importance of science and technology for China\\'s economic growth.\\n\\n\\n=== Late 1970s to early 2010s ===\\nArtificial intelligence research and development did not start until the late 1970s after Deng Xiaoping\\'s economic reforms. While there was a lack of AI-related research between the 1950s and 1960s, some scholars believe this is due to the influence of cybernetics from the Soviet Union despite the Sino-Soviet split during the late 1950s and early 1960s. In the 1980s, a group of Chinese scientists launched AI research led by Qian Xuesen and Wu Wenjun. However, during the time, China\\'s society still had a generally conservative view towards AI. Early AI development in China was difficult so China\\'s government approached these challenges by sending Chinese scholars overseas to study AI and further providing government funds for research projects. The Chinese Association for Artificial Intelligence (CAAI) was founded in September 1981 and was authorized by the Ministry of Civil Affairs. The first chairman of the executive committee was Qin Yuanxun, who received a PhD in philosophy from Harvard University.  In 1987, China\\'s first research publication on artificial intelligence was published by Tsinghua University. Beginning in 1993, smart automation and intelligence have been part of China\\'s national technology plan.\\nSince the 2000s, the Chinese government has further expanded its research and development funds for AI and the number of government-sponsored research projects has dramatically increased. In 200'),\n",
       " Document(metadata={'title': 'Artificial intelligence in India', 'summary': \"The Artificial Intelligence (AI) market in India is projected to reach $8 billion by 2025, growing at a compound annual growth rate (CAGR) of over 40% from 2020 to 2025. This growth is part of the broader AI boom, a global period of rapid technological advancements starting in the late 2010s and gaining prominence in the early 2020s. Globally, breakthroughs in protein folding by Google DeepMind and the rise of generative AI models from OpenAI have defined this era. In India, the development of AI has been similarly transformative, with applications in healthcare, finance, and education, bolstered by government initiatives like NITI Aayog's 2018 National Strategy for Artificial Intelligence.\\nWhile AI presents significant opportunities for economic growth and social development in India, challenges such as data privacy concerns, skill shortages, and ethical considerations need to be addressed for responsible AI deployment.\", 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence_in_India'}, page_content=\"The Artificial Intelligence (AI) market in India is projected to reach $8 billion by 2025, growing at a compound annual growth rate (CAGR) of over 40% from 2020 to 2025. This growth is part of the broader AI boom, a global period of rapid technological advancements starting in the late 2010s and gaining prominence in the early 2020s. Globally, breakthroughs in protein folding by Google DeepMind and the rise of generative AI models from OpenAI have defined this era. In India, the development of AI has been similarly transformative, with applications in healthcare, finance, and education, bolstered by government initiatives like NITI Aayog's 2018 National Strategy for Artificial Intelligence.\\nWhile AI presents significant opportunities for economic growth and social development in India, challenges such as data privacy concerns, skill shortages, and ethical considerations need to be addressed for responsible AI deployment.\\n\\n\\n== History ==\\n\\n\\n=== Early days (1960s-1980s) ===\\nThe TIFRAC (Tata Institute of Fundamental Research Automatic Calculator) was designed and developed by a team led by Rangaswamy Narasimhan between 1954 and 1960. He worked on pattern recognition from 1961 to 1964 at the University of Illinois Urbana-Champaign's Digital Computer Laboratory. In order to conduct research on database technology, computer networking, computer graphics, and systems software, he and M. G. K. Menon founded the National Centre for Software Development and Computing Techniques. In 1965, he established the Computer Society of India and supervised the initial research work on AI at Tata Institute of Fundamental Research. Jagdish Lal launched the first computer science program in 1976 at Motilal Nehru Regional Engineering College. H. K. Kesavan from the University of Waterloo and Vaidyeswaran Rajaraman from the University of Wisconsin–Madison joined the IIT Kanpur Electrical Engineering Department in 1963–1964 as Assistant Professor and Head of Department, respectively. H.N. Mahabala, who was employed at Bendix Corporation's Computer Division, joined the department in 1965. He previously worked with Marvin Minsky. The IIT Kanpur Computer Center was led by H. K. Kesavan, with Vaidyeswaran Rajaraman serving as his deputy. Kesavan informally permitted Rajaraman and Mahabala to introduce artificial intelligence into computer science classes. The computer science program was approved by IIT Kanpur in 1971 and split out from the electrical engineering department. In 1973, an IBM System/370 Model 155 was installed at IIT Madras. John McCarthy, head of the Artificial Intelligence Laboratory at Stanford University visited IIT Kanpur in 1971. He donated PDP-1 with a time-sharing operating system. During the 1970s, the balance of payments deficit in India restricted import of computers. The Department of Computer Science and Automation at the Indian Institute of Science established in 1969, played an important role in nurturing the development of data science and artificial intelligence in India. First course on AI was introduced in 1970's by G. Krishna. B. L. Deekshatulu introduced the first course on pattern recognition in early 1970s.\\n\\n\\n=== Foundation phase ===\\n\\n\\n==== 1980s ====\\nIn the 1980s, the Indian Statistical Institute's Optical Character Recognition Project was one of the country's first attempts at studying artificial intelligence and machine learning. OCR technology has benefited greatly from the work of ISI's Computer Vision and Pattern Recognition Unit, which is headed by Bidyut Baran Chaudhuri. He also contributed in the development of computer vision and digital image processing. As part of the Indian Fifth Generation Computer Systems Research Programme, the Department of Electronics, with support from the United Nations Development Programme, initiated the Knowledge Based Computer Systems Project in 1986, marking the beginning of India's first major AI research program. Prime Minister Rajiv Gandhi requested that the Department of Ele\"),\n",
       " Document(metadata={'title': 'Artificial intelligence in video games', 'summary': \"In video games, artificial intelligence (AI) is used to generate responsive, adaptive or intelligent behaviors primarily in non-playable characters (NPCs) similar to human-like intelligence. Artificial intelligence has been an integral part of video games since their inception in 1948, first seen in the game Nim. AI in video games is a distinct subfield and differs from academic AI. It serves to improve the game-player experience rather than machine learning or decision making. During the golden age of arcade video games the idea of AI opponents was largely popularized in the form of graduated difficulty levels, distinct movement patterns, and in-game events dependent on the player's input. Modern games often implement existing techniques such as pathfinding and decision trees to guide the actions of NPCs. AI is often used in mechanisms which are not immediately visible to the user, such as data mining and procedural-content generation. One of the most infamous examples of this NPC technology and gradual difficulty levels can be found in the game Mike Tyson's Punch-Out!! (1987).  \\nIn general, game AI does not, as might be thought and sometimes is depicted to be the case, mean a realization of an artificial person corresponding to an NPC in the manner of the Turing test or an artificial general intelligence.\", 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence_in_video_games'}, page_content='In video games, artificial intelligence (AI) is used to generate responsive, adaptive or intelligent behaviors primarily in non-playable characters (NPCs) similar to human-like intelligence. Artificial intelligence has been an integral part of video games since their inception in 1948, first seen in the game Nim. AI in video games is a distinct subfield and differs from academic AI. It serves to improve the game-player experience rather than machine learning or decision making. During the golden age of arcade video games the idea of AI opponents was largely popularized in the form of graduated difficulty levels, distinct movement patterns, and in-game events dependent on the player\\'s input. Modern games often implement existing techniques such as pathfinding and decision trees to guide the actions of NPCs. AI is often used in mechanisms which are not immediately visible to the user, such as data mining and procedural-content generation. One of the most infamous examples of this NPC technology and gradual difficulty levels can be found in the game Mike Tyson\\'s Punch-Out!! (1987).  \\nIn general, game AI does not, as might be thought and sometimes is depicted to be the case, mean a realization of an artificial person corresponding to an NPC in the manner of the Turing test or an artificial general intelligence.\\n\\n\\n== Overview ==\\nThe term game AI is used to refer to a broad set of algorithms that also include techniques from control theory, robotics, computer graphics and computer science in general, and so video game AI may often not constitute \"true AI\" in that such techniques do not necessarily facilitate computer learning or other standard criteria, only constituting \"automated computation\" or a predetermined and limited set of responses to a predetermined and limited set of inputs.\\nMany industries and corporate voices argue that game AI has come a long way in the sense that it has revolutionized the way humans interact with all forms of technology, although many expert researchers are skeptical of such claims, and particularly of the notion that such technologies fit the definition of \"intelligence\" standardly used in the cognitive sciences. Industry voices make the argument that AI has become more versatile in the way we use all technological devices for more than their intended purpose because the AI allows the technology to operate in multiple ways, allegedly developing their own personalities and carrying out complex instructions of the user.\\nPeople in the field of AI have argued that video game AI is not true intelligence, but an advertising buzzword used to describe computer programs that use simple sorting and matching algorithms to create the illusion of intelligent behavior while bestowing software with a misleading aura of scientific or technological complexity and advancement. Since game AI for NPCs is centered on appearance of intelligence and good gameplay within environment restrictions, its approach is very different from that of traditional AI.\\n\\n\\n== History ==\\nGame playing was an area of research in AI from its inception. One of the first examples of AI is the computerized game of Nim made in 1951 and published in 1952. Despite being advanced technology in the year it was made, 20 years before Pong, the game took the form of a relatively small box and was able to regularly win games even against highly skilled players of the game. In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. These were among the first computer programs ever written. Arthur Samuel\\'s checkers program, developed in the middle 1950s and early 1960s, eventually achieved sufficient skill to challenge a respectable amateur. Work on checkers and chess would culminate in the defeat of Garry Kasparov by IBM\\'s Deep Blue computer in 1997. The first video games developed in the 1960s and early 1970s, like Spacewar!, Pong, and Gotcha (1973), we'),\n",
       " Document(metadata={'title': 'Existential risk from artificial intelligence', 'summary': 'Existential risk from artificial intelligence refers to the idea that substantial progress in artificial general intelligence (AGI) could lead to human extinction or an irreversible global catastrophe.\\nOne argument for the importance of this risk references how human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass human intelligence and become superintelligent, it might become uncontrollable. Just as the fate of the mountain gorilla depends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence.\\nThe plausibility of existential catastrophe due to AI is widely debated. It hinges in part on whether AGI or superintelligence are achievable, the speed at which dangerous capabilities and behaviors emerge, and whether practical scenarios for AI takeovers exist. Concerns about superintelligence have been voiced by computer scientists and tech CEOs such as Geoffrey Hinton, Yoshua Bengio, Alan Turing, Elon Musk, and OpenAI CEO Sam Altman. In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe. In 2023, hundreds of AI experts and other notable figures signed a statement declaring, \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak and United Nations Secretary-General António Guterres called for an increased focus on global AI regulation.\\nTwo sources of concern stem from the problems of AI control and alignment. Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints. In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.\\nA third source of concern is the possibility of a sudden \"intelligence explosion\" that catches humanity unprepared. In this scenario, an AI more intelligent than its creators would be able to recursively improve itself at an exponentially increasing rate, improving too quickly for its handlers or society at large to control. Empirically, examples like AlphaZero, which taught itself to play Go and quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such machine learning systems do not recursively improve their fundamental architecture.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence'}, page_content='Existential risk from artificial intelligence refers to the idea that substantial progress in artificial general intelligence (AGI) could lead to human extinction or an irreversible global catastrophe.\\nOne argument for the importance of this risk references how human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass human intelligence and become superintelligent, it might become uncontrollable. Just as the fate of the mountain gorilla depends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence.\\nThe plausibility of existential catastrophe due to AI is widely debated. It hinges in part on whether AGI or superintelligence are achievable, the speed at which dangerous capabilities and behaviors emerge, and whether practical scenarios for AI takeovers exist. Concerns about superintelligence have been voiced by computer scientists and tech CEOs such as Geoffrey Hinton, Yoshua Bengio, Alan Turing, Elon Musk, and OpenAI CEO Sam Altman. In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe. In 2023, hundreds of AI experts and other notable figures signed a statement declaring, \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak and United Nations Secretary-General António Guterres called for an increased focus on global AI regulation.\\nTwo sources of concern stem from the problems of AI control and alignment. Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints. In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.\\nA third source of concern is the possibility of a sudden \"intelligence explosion\" that catches humanity unprepared. In this scenario, an AI more intelligent than its creators would be able to recursively improve itself at an exponentially increasing rate, improving too quickly for its handlers or society at large to control. Empirically, examples like AlphaZero, which taught itself to play Go and quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such machine learning systems do not recursively improve their fundamental architecture.\\n\\n\\n== History ==\\nOne of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler, who wrote in his 1863 essay Darwin among the Machines:\\n\\nThe upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question.\\nIn 1951, foundational computer scientist Alan Turing wrote the article \"Intelligent Machinery, A Heretical Theory\", in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\\n\\nLet us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen thei'),\n",
       " Document(metadata={'title': 'Applications of artificial intelligence', 'summary': 'ههههه🇸🇩￼🇵🇸🏹\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence'}, page_content=\"ههههه🇸🇩￼🇵🇸🏹\\n\\n\\n== Internet and e-commerce ==\\n\\n\\n=== Web feeds and posts ===\\nMachine learning has been used for recommendation systems in determining which posts should show up in social media feeds. Various types of social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.\\nAI has been used to customize shopping options and personalize offers. Online gambling companies have used AI for targeting gamblers.\\n\\n\\n=== Virtual assistants and search ===\\n\\nIntelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.\\nBing Chat has used artificial intelligence as part of its search engine.\\n\\n\\n=== Spam filtering ===\\n\\nMachine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements. Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails. These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.\\n\\n\\n=== Language translation ===\\n\\nSpeech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another. \\nAI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator. Additionally, research and development are in progress to decode and conduct animal communication.\\nMeaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.\\n\\n\\n=== Facial recognition and image labeling ===\\n\\nAI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.\\nImage labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people. Facebook's DeepFace identifies human faces in digital images.\\n\\n\\n== Games and entertainment ==\\n\\nGames have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero).\\nKuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool. Character.ai is another example of a chatbot being used for recreation.\\n\\n\\n== Economic and social challenges ==\\nAI for Good is a platform launched in 2017 by the International Telecommunication Union (ITU) agency of the United Nations (UN). The goal of the platform is to use AI to help achieve the UN's Sustainable Development Goals.\\nThe University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high povert\"),\n",
       " Document(metadata={'title': 'Artificial intelligence art', 'summary': 'Artificial intelligence art is visual artwork created or enhanced through the use of artificial intelligence (AI) programs.\\nArtists began to create artificial intelligence art in the mid to late 20th century when the discipline was founded. Throughout its history, artificial intelligence art has raised many philosophical concerns related to the human mind, artificial beings, and what can be considered art in a human–AI collaboration. Since the 20th century, artists have used AI to create art, some of which has been exhibited in museums and won awards.\\nDuring the AI boom of the 2020s, text-to-image models such as Midjourney, DALL-E, Stable Diffusion, and FLUX.1 became widely available to the public, allowing non-artists to quickly generate imagery with little effort. Commentary about AI art in the 2020s has often focused on issues related to copyright, deception, defamation, and its impact on more traditional artists, including technological unemployment. Opinions have also risen on the possible effect AI generated art might have on creativity.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence_art'}, page_content='Artificial intelligence art is visual artwork created or enhanced through the use of artificial intelligence (AI) programs.\\nArtists began to create artificial intelligence art in the mid to late 20th century when the discipline was founded. Throughout its history, artificial intelligence art has raised many philosophical concerns related to the human mind, artificial beings, and what can be considered art in a human–AI collaboration. Since the 20th century, artists have used AI to create art, some of which has been exhibited in museums and won awards.\\nDuring the AI boom of the 2020s, text-to-image models such as Midjourney, DALL-E, Stable Diffusion, and FLUX.1 became widely available to the public, allowing non-artists to quickly generate imagery with little effort. Commentary about AI art in the 2020s has often focused on issues related to copyright, deception, defamation, and its impact on more traditional artists, including technological unemployment. Opinions have also risen on the possible effect AI generated art might have on creativity.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\n\\nThe concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. Early experiments were driven by the idea that computers, beyond performing logical operations, could generate aesthetically pleasing works, offering a new dimension to creativity. The tradition of creative automatons has flourished throughout history, such as Maillardet\\'s automaton, created around 1800 and capable of creating multiple drawings and poems stored in its \"cams”, the brass disks that hold memory.\\nAlong with this, Ada Lovelace, typically known for her work on the analytical engine, in her notes, begins to conceptualize the idea \"computing operations\" could be used to generate music and poems. This concept resulted in what is now referred to as \"The Lovelace Effect,\" which gives a concrete set of tools to analyze situations where a computer\\'s behavior is viewed by users as creative. However, Lovelace also discusses a concept in her notes that is known as \"The Lovelace Objection,\" where she argues that machines have \"no pretensions whatever to originate anything,\" which is a direct contradiction to the idea of artificial intelligence and creative machines.\\nIn 1950, with the publication of Alan Turing\\'s paper \"Computing Machinery and Intelligence\", there was a shift from defining intelligence in regards to machines in abstract terms to evaluating whether a machine can mimic human behavior and responses convincingly. Shortly after, the academic discipline of artificial intelligence was founded at a research workshop at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Since its founding, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity.\\n\\n\\n=== 1950s to 2000s: Early implementations ===\\nSince the founding of AI in the 1950s, artists and researchers have used artificial intelligence to create artistic works. These works were sometimes referred to as algorithmic art, computer art, digital art, or new media art.\\nOne of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego. AARON uses a symbolic rule-based approach to generate technical images in the era of GOFAI programming, and it was developed by Cohen with the goal of being able to code the act of drawing. In its earliest form, AARON created abstract black-and-white drawings which would later be finished by Cohen painting them. Throughout the years, he also began'),\n",
       " Document(metadata={'title': 'Ethics of artificial intelligence', 'summary': 'The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \\nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. \\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence'}, page_content='The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \\nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. \\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\\n\\n\\n== Machine ethics ==\\n\\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.\\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI\\'s decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions. And large language models are capable of approximating human moral judgments. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human \\'weaknesses\\' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\\nIn Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms, while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".\\n\\n\\n=== Robot ethics ===\\n\\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.\\n\\n\\n=== Ethical principles ===\\nIn the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, tr'),\n",
       " Document(metadata={'title': 'Technological singularity', 'summary': 'The technological singularity—or simply the singularity—is a hypothetical point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable consequences for human civilization. According to the most popular version of the singularity hypothesis, I. J. Good\\'s intelligence explosion model of 1965, an upgradable intelligent agent could eventually enter a positive feedback loop of successive self-improvement cycles; more intelligent generations would appear more and more rapidly, causing a rapid increase (\"explosion\") in intelligence which would culminate in a powerful superintelligence, far surpassing all human intelligence.\\nfuture\\nThe Hungarian-American mathematician John von Neumann (1903-1957) became the first known person to use the concept of a \"singularity\" in the technological context.\\nAlan Turing, often regarded as the father of modern computer science, laid a crucial foundation for the contemporary discourse on the technological singularity. His pivotal 1950 paper, \"Computing Machinery and Intelligence,\" introduces the idea of a machine\\'s ability to exhibit intelligent behavior equivalent to or indistinguishable from that of a human.\\nStanislaw Ulam reported in 1958 an earlier discussion with von Neumann \"centered on the accelerating progress of technology and changes in human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue\". Subsequent authors have echoed this viewpoint.\\nThe concept and the term \"singularity\" were popularized by Vernor Vinge – first in 1983 (in an article that claimed that once humans create intelligences greater than their own, there will be a technological and social transition similar in some sense to \"the knotted space-time at the center of a black hole\",) and later in his 1993 essay The Coming Technological Singularity, (in which he wrote that it would signal the end of the human era, as the new superintelligence would continue to upgrade itself and would advance technologically at an incomprehensible rate). He wrote that he would be surprised if it occurred before 2005 or after 2030. Another significant contributor to wider circulation of the notion was Ray Kurzweil\\'s 2005 book The Singularity Is Near, predicting singularity by 2045.\\nSome scientists, including Stephen Hawking, have expressed concern that artificial superintelligence (ASI) could result in human extinction. The consequences of a technological singularity and its potential benefit or harm to the human race have been intensely debated.\\nProminent technologists and academics dispute the plausibility of a technological singularity and the associated artificial intelligence explosion, including Paul Allen, Jeff Hawkins, John Holland, Jaron Lanier, Steven Pinker, Theodore Modis, Gordon Moore, and Roger Penrose. One claim made was that artificial intelligence growth is likely to run into decreasing returns instead of accelerating ones, as was observed in previously developed human technologies.', 'source': 'https://en.wikipedia.org/wiki/Technological_singularity'}, page_content='The technological singularity—or simply the singularity—is a hypothetical point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable consequences for human civilization. According to the most popular version of the singularity hypothesis, I. J. Good\\'s intelligence explosion model of 1965, an upgradable intelligent agent could eventually enter a positive feedback loop of successive self-improvement cycles; more intelligent generations would appear more and more rapidly, causing a rapid increase (\"explosion\") in intelligence which would culminate in a powerful superintelligence, far surpassing all human intelligence.\\nfuture\\nThe Hungarian-American mathematician John von Neumann (1903-1957) became the first known person to use the concept of a \"singularity\" in the technological context.\\nAlan Turing, often regarded as the father of modern computer science, laid a crucial foundation for the contemporary discourse on the technological singularity. His pivotal 1950 paper, \"Computing Machinery and Intelligence,\" introduces the idea of a machine\\'s ability to exhibit intelligent behavior equivalent to or indistinguishable from that of a human.\\nStanislaw Ulam reported in 1958 an earlier discussion with von Neumann \"centered on the accelerating progress of technology and changes in human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue\". Subsequent authors have echoed this viewpoint.\\nThe concept and the term \"singularity\" were popularized by Vernor Vinge – first in 1983 (in an article that claimed that once humans create intelligences greater than their own, there will be a technological and social transition similar in some sense to \"the knotted space-time at the center of a black hole\",) and later in his 1993 essay The Coming Technological Singularity, (in which he wrote that it would signal the end of the human era, as the new superintelligence would continue to upgrade itself and would advance technologically at an incomprehensible rate). He wrote that he would be surprised if it occurred before 2005 or after 2030. Another significant contributor to wider circulation of the notion was Ray Kurzweil\\'s 2005 book The Singularity Is Near, predicting singularity by 2045.\\nSome scientists, including Stephen Hawking, have expressed concern that artificial superintelligence (ASI) could result in human extinction. The consequences of a technological singularity and its potential benefit or harm to the human race have been intensely debated.\\nProminent technologists and academics dispute the plausibility of a technological singularity and the associated artificial intelligence explosion, including Paul Allen, Jeff Hawkins, John Holland, Jaron Lanier, Steven Pinker, Theodore Modis, Gordon Moore, and Roger Penrose. One claim made was that artificial intelligence growth is likely to run into decreasing returns instead of accelerating ones, as was observed in previously developed human technologies.\\n\\n\\n== Intelligence explosion ==\\n\\nAlthough technological progress has been accelerating in most areas, it has been limited by the basic intelligence of the human brain, which has not, according to Paul R. Ehrlich, changed significantly for millennia. However, with the increasing power of computers and other technologies, it might eventually be possible to build a machine that is significantly more intelligent than humans.\\nIf a superhuman intelligence were to be invented—either through the amplification of human intelligence or through artificial intelligence—it would, in theory, vastly improve over human problem-solving and inventive skills. Such an AI is referred to as Seed AI because if an AI were created with engineering capabilities that matched or surpassed those of its human creators, it would have the potential to autonomously improve its own software and hardware to design an even mor'),\n",
       " Document(metadata={'title': 'Frame (artificial intelligence)', 'summary': 'Frames are an artificial intelligence data structure used to divide knowledge into substructures by representing \"stereotyped situations\".\\nThey were proposed by Marvin Minsky in his 1974 article \"A Framework for Representing Knowledge\". Frames are the primary data structure used in artificial intelligence frame languages; they are stored as ontologies of sets.\\nFrames are also an extensive part of knowledge representation and reasoning schemes. They were originally derived from semantic networks and are therefore part of structure-based knowledge representations.\\nAccording to Russell and Norvig\\'s Artificial Intelligence: A Modern Approach, structural representations assemble \"[...]facts about particular objects and event types and arrange the types into a large taxonomic hierarchy analogous to a biological taxonomy\".', 'source': 'https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)'}, page_content='Frames are an artificial intelligence data structure used to divide knowledge into substructures by representing \"stereotyped situations\".\\nThey were proposed by Marvin Minsky in his 1974 article \"A Framework for Representing Knowledge\". Frames are the primary data structure used in artificial intelligence frame languages; they are stored as ontologies of sets.\\nFrames are also an extensive part of knowledge representation and reasoning schemes. They were originally derived from semantic networks and are therefore part of structure-based knowledge representations.\\nAccording to Russell and Norvig\\'s Artificial Intelligence: A Modern Approach, structural representations assemble \"[...]facts about particular objects and event types and arrange the types into a large taxonomic hierarchy analogous to a biological taxonomy\".\\n\\n\\n== Frame structure ==\\nThe frame contains information on how to use the frame, what to expect next, and what to do when these expectations are not met.\\nSome information in the frame is generally unchanged while other information, stored in \"terminals\", usually change. Terminals can be considered as variables.\\nTop-level frames carry information, that is always true about the problem in hand, however, terminals do not have to be true. Their value might change with the new information encountered. Different frames may share the same terminals.\\nEach piece of information about a particular frame is held in a slot.\\nThe information can contain:\\n\\nFacts or Data\\nValues (called facets)\\nProcedures (also called procedural attachments)\\nIF-NEEDED: deferred evaluation\\nIF-ADDED: updates linked information\\nDefault Values\\nFor Data\\nFor Procedures\\nOther Frames or Subframes\\n\\n\\n== Features and advantages ==\\nA frame\\'s terminals are already filled with default values, which is based on how the human mind works.\\nFor example, when a person is told \"a boy kicks a ball\", most people will visualize a particular ball (such as a familiar soccer ball) rather than imagining some abstract ball with no attributes.\\nOne particular strength of frame-based knowledge representations is that, unlike semantic networks, they allow for exceptions in particular instances. This gives frames a degree of flexibility that allows representations to reflect real-world phenomena more accurately.\\nLike semantic networks, frames can be queried using spreading activation. Following the rules of inheritance, any value given to a slot that is inherited by subframes will be updated (IF-ADDED) to the corresponding slots in the subframes and any new instances of a particular frame will feature that new value as the default.\\nBecause frames are based on structures, it is possible to generate a semantic network given a set of frames even though it lacks explicit arcs. References to Noam Chomsky and his generative grammar of 1950 are generally missing from Minsky\\'s work.\\nThe simplified structures of frames allow for easy analogical reasoning, a much prized feature in any intelligent agent. The procedural attachments provided by frames also allow a degree of flexibility that makes for a more realistic representation and gives a natural affordance for programming applications.\\n\\n\\n== Example ==\\nWorth noticing here is the easy analogical reasoning (comparison) that can be done between a boy and a monkey just by having similarly named slots.\\nAlso notice that Alex, an instance of a boy, inherits default values like \"Sex\" from the more general parent object Boy,\\nbut the boy may also have different instance values in the form of exceptions such as the number of legs.\\n\\n\\n== Frame language ==\\nA frame language is a technology used for knowledge representation in artificial intelligence. They are similar to class hierarchies in object-oriented languages although their fundamental design goals are different.\\nFrames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding. Frames originated in AI research and objects primaril'),\n",
       " Document(metadata={'title': 'Artificial intelligence in healthcare', 'summary': 'Artificial intelligence in healthcare is the application of artificial intelligence (AI) to analyze and understand complex medical and healthcare data. In some cases, it can exceed or augment human capabilities by providing better or faster ways to diagnose, treat, or prevent disease.\\nAs widespread use of AI in healthcare is relatively new, research is ongoing into its application in various subdisciplines of medicine and related industries. AI programs are applied to practices such as diagnostics, treatment protocol development, drug development, personalized medicine, and patient monitoring and care. Because radiographs are the most common imaging tests conducted in radiology departments, the potential for AI to help with triage and interpretation of radiographs is particularly noteworthy.\\nUsing AI also presents unprecedented ethical concerns related to issues such as data privacy, automation of jobs, and amplifying already existing biases. Furthermore, new technologies such as AI are often resisted by healthcare leaders, leading to slow and erratic adoption. In contrast, there are also several cases where AI has been put to use in healthcare without proper testing. A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic. Moreover, meta-studies have found that the scientific literature on AI in healthcare often suffers from a lack of reproducibility.', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare'}, page_content=\"Artificial intelligence in healthcare is the application of artificial intelligence (AI) to analyze and understand complex medical and healthcare data. In some cases, it can exceed or augment human capabilities by providing better or faster ways to diagnose, treat, or prevent disease.\\nAs widespread use of AI in healthcare is relatively new, research is ongoing into its application in various subdisciplines of medicine and related industries. AI programs are applied to practices such as diagnostics, treatment protocol development, drug development, personalized medicine, and patient monitoring and care. Because radiographs are the most common imaging tests conducted in radiology departments, the potential for AI to help with triage and interpretation of radiographs is particularly noteworthy.\\nUsing AI also presents unprecedented ethical concerns related to issues such as data privacy, automation of jobs, and amplifying already existing biases. Furthermore, new technologies such as AI are often resisted by healthcare leaders, leading to slow and erratic adoption. In contrast, there are also several cases where AI has been put to use in healthcare without proper testing. A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic. Moreover, meta-studies have found that the scientific literature on AI in healthcare often suffers from a lack of reproducibility.\\n\\n\\n== Applications in healthcare systems ==\\n\\n\\n=== Disease diagnosis ===\\nAccurate and early diagnosis of diseases is still a challenge in healthcare. Recognising medical conditions and their symptoms is a complex problem. AI can assist clinicians with its data processing capabilities to save time and improve accuracy. Through the use of machine learning, artificial intelligence can be able to substantially aid doctors in patient diagnosis through the analysis of mass electronic health records (EHRs). AI can help early prediction, for example, of Alzheimer's disease and dementias, by looking through large numbers of similar cases and possible treatments.\\nDoctors' decision making could also be supported by AI in urgent situations, for example in the emergency department. Here AI algorithms can help prioritize more serious cases and reduce waiting time. Decision support systems augmented with AI can offer real-time suggestions and faster data interpretation to aid the decisions made by healthcare professionals.\\nIn 2023 a study reported higher satisfaction rates with ChatGPT-generated responses compared with those from physicians for medical questions posted on Reddit’s r/AskDocs. Evaluators preferred ChatGPT's responses to physician responses in 78.6% of 585 evaluations, noting better quality and empathy. The authors noted that these were isolated questions taken from an online forum, not in the context of an established patient-physician relationship. Moreover, responses were not graded on the accuracy of medical information, and some have argued that the experiment was not properly blinded, with the evaluators being coauthors of the study.\\nRecent developments in statistical physics, machine learning, and inference algorithms are also being explored for their potential in improving medical diagnostic approaches.\\n\\n\\n=== Electronic health records ===\\nElectronic health records (EHR) are crucial to the digitalization and information spread of the healthcare industry. Now that around 80% of medical practices use EHR, some anticipate the use of artificial intelligence to interpret the records and provide new information to physicians.\\nOne application uses natural language processing (NLP) to make more succinct reports that limit the variation between medical terms by matching similar medical terms. For example, the term heart attack and myocardial infarction mean the same things, but physicians may use one over the over based on personal preferences. N\"),\n",
       " Document(metadata={'title': 'Timeline of artificial intelligence', 'summary': 'This is a timeline of artificial intelligence, sometimes alternatively called synthetic intelligence.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence'}, page_content='This is a timeline of artificial intelligence, sometimes alternatively called synthetic intelligence.\\n\\n\\n== Antiquity, Classical and Medieval eras ==\\n\\n\\n== 1600-1900 ==\\n\\n\\n== 20th century ==\\n\\n\\n=== 1901–1950 ===\\n\\n\\n=== 1950s ===\\n\\n\\n=== 1960s ===\\n\\n\\n=== 1970s ===\\n\\n\\n=== 1980s ===\\n\\n\\n=== 1990s ===\\n\\n\\n== 21st century ==\\n\\n\\n=== 2000s ===\\n\\n\\n=== 2010s ===\\n\\n\\n=== 2020s ===\\n\\n\\n== See also ==\\nTimeline of machine translation\\nTimeline of machine learning\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Sources ==\\nBuchanan, Bruce G. (2005), \"A (Very) Brief History of Artificial Intelligence\" (PDF), AI Magazine, pp. 53–60, archived from the original (PDF) on 26 September 2007, retrieved 30 August 2007\\nChristian, Brian (2020). The Alignment Problem: Machine learning and human values. W. W. Norton & Company. ISBN 978-0-393-86833-3. OCLC 1233266753.\\nCrevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.\\nLinsky, Bernard; Irvine, Andrew David (Spring 2022). Edward N. Zalta (ed.). \"Principia Mathematica\". The Stanford Encyclopedia of Philosophy.\\nMcCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2\\nNeedham, Joseph (1986). Science and Civilization in China: Volume 2. Taipei: Caves Books Ltd.\\nRussell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0134610993. LCCN 20190474.\\nSamuel, Arthur L. (July 1959), \"Some studies in machine learning using the game of checkers\", IBM Journal of Research and Development, 3 (3): 210–219, CiteSeerX 10.1.1.368.2254, doi:10.1147/rd.33.0210, S2CID 2126705, archived from the original on 3 March 2016, retrieved 20 August 2007\\nSchmidhuber, Jürgen (2022). \"Annotated History of Modern AI and Deep Learning\".\\nWong, Matteo (19 May 2023), \"ChatGPT Is Already Obsolete\", The Atlantic\\n\\n\\n== Further reading ==\\nBerlinski, David (2000), The Advent of the Algorithm, Harcourt Books\\nBrooks, Rodney (1990), \"Elephants Don\\'t Play Chess\" (PDF), Robotics and Autonomous Systems, 6 (1–2): 3–15, CiteSeerX 10.1.1.588.7539, doi:10.1016/S0921-8890(05)80025-9, retrieved 30 August 2007\\nDarrach, Brad (20 November 1970), \"Meet Shakey, the First Electronic Person\", Life Magazine, pp. 58–68\\nDoyle, J. (1983), \"What is rational psychology? Toward a modern mental philosophy\", AI Magazine, vol. 4, no. 3, pp. 50–53\\nDreyfus, Hubert (1972), What Computers Can\\'t Do, MIT Press\\nFeigenbaum, Edward A.; McCorduck, Pamela (1983), The Fifth Generation: Artificial Intelligence and Japan\\'s Computer Challenge to the World, Michael Joseph, ISBN 978-0-7181-2401-4\\nFeigenbaum, Edward; Feldman, Julian, eds. (1963), Computers and thought (1 ed.), New York: McGraw-Hill, OCLC 593742426\\nHobbes (1651), Leviathan\\nHofstadter, Douglas (1980), Gödel, Escher, Bach: an Eternal Golden Braid\\nHowe, J. (November 1994), Artificial Intelligence at Edinburgh University: a Perspective, retrieved 30 August 2007\\nKaplan, Andreas; Haenlein, Michael (2018), \"Siri, Siri in my Hand, who\\'s the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence\", Business Horizons, 62: 15–25, doi:10.1016/j.bushor.2018.08.004, S2CID 158433736\\nKurzweil, Ray (2005), The Singularity is Near, Viking Press\\nLakoff, George (1987), Women, Fire, and Dangerous Things: What Categories Reveal About the Mind, University of Chicago Press., ISBN 978-0-226-46804-4\\nLenat, Douglas; Guha, R. V. (1989), Building Large Knowledge-Based Systems, Addison-Wesley\\nLevitt, Gerald M. (2000), The Turk, Chess Automaton, Jefferson, N.C.: McFarland, ISBN 978-0-7864-0778-1\\nLighthill, Professor Sir James (1973), \"Artificial Intelligence: A General Survey\", Artificial Intelligence: a paper symposium, Science Research Council\\nLucas, John (1961), Minds, Machines and Gödel, archived from the original on 19 August 2007, retrieved 24 July 2007\\nMcCarthy, John; Minsky, Marvin; Rochester, Nathan; Shannon, Claude (1955), A Proposal for the Dartm'),\n",
       " Document(metadata={'title': 'Regulation of artificial intelligence', 'summary': 'Regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI). It is part of the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions worldwide, including for international organizations without direct enforcement power like the IEEE or the OECD.\\nSince 2016, numerous AI ethics guidelines have been published in order to maintain social control over the technology. Regulation is deemed necessary to both foster AI innovation and manage associated risks.\\nFurthermore, organizations deploying AI have a central role to play in creating and implementing trustworthy AI, adhering to established principles, and taking accountability for mitigating risks.\\nRegulating AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence'}, page_content='Regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI). It is part of the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions worldwide, including for international organizations without direct enforcement power like the IEEE or the OECD.\\nSince 2016, numerous AI ethics guidelines have been published in order to maintain social control over the technology. Regulation is deemed necessary to both foster AI innovation and manage associated risks.\\nFurthermore, organizations deploying AI have a central role to play in creating and implementing trustworthy AI, adhering to established principles, and taking accountability for mitigating risks.\\nRegulating AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.\\n\\n\\n== Background ==\\nAccording to Stanford University\\'s 2023 AI Index, the annual number of bills mentioning \"artificial intelligence\" passed in 127 surveyed countries jumped from one in 2016 to 37 in 2022.\\nIn 2017, Elon Musk called for regulation of AI development. According to NPR, the Tesla CEO was \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believed the risks of going completely without oversight are high: \"Normally the way regulations are set up is when a bunch of bad things happen, there\\'s a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilization.\" In response, some politicians expressed skepticism about the wisdom of regulating a technology that is still in development. Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich has argued that AI is in its infancy and that it is too early to regulate the technology. Many tech companies oppose the harsh regulation of AI and \"While some of the companies have said they welcome rules around A.I., they have also argued against tough regulations akin to those being created in Europe\"  Instead of trying to regulate the technology itself, some scholars suggested developing common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.\\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\\n\\n\\n== Perspectives ==\\nThe regulation of artificial intelligences is the development of public sector policies and laws for promoting and regulating AI. Regulation is now generally considered necessary to both encourage AI and manage associated risks. Public administration and policy considerations generally focus on the technical and economic implications and on trustworthy and human-centered AI systems, although regulation of artificial superintelligences is also considered. The basic approach to regulation focuses on the risks and biases of machine-learning algorithms, at the level of the input data, algorithm testing, and decision model. It also focuses on the explainability of the outputs.\\nThere have been both hard law and soft law proposals to regulate AI. Some legal scholars have noted that hard law approaches to AI regulation have substantial challenges. Among the challenges, AI technology is rapid'),\n",
       " Document(metadata={'title': 'Artificial Intelligence Act', 'summary': \"The Artificial Intelligence Act (AI Act) is a European Union regulation concerning artificial intelligence (AI). It establishes a common regulatory and legal framework for AI within the European Union (EU). It came into force on 1 August 2024, with provisions that shall come into operation gradually over the following 6 to 36 months.\\nIt covers all types of AI across a broad range of sectors, with exceptions for AI systems used solely for military, national security, research and non-professional purposes. As a piece of product regulation, it does not confer rights on individuals, but regulates the providers of AI systems and entities using AI in a professional context.\\nThe Act classifies non-exempt AI applications by their risk of causing harm. There are four levels – unacceptable, high, limited, minimal – plus an additional category for general-purpose AI. \\n\\nApplications with unacceptable risks are banned.\\nHigh-risk applications must comply with security, transparency and quality obligations, and undergo conformity assessments.\\nLimited-risk applications only have transparency obligations.\\nMinimal-risk applications are not regulated.\\nFor general-purpose AI, transparency requirements are imposed, with reduced requirements for open source models, and additional evaluations for high-capability models.\\nThe Act also creates a European Artificial Intelligence Board to promote national cooperation and ensure compliance with the regulation. Like the EU's General Data Protection Regulation, the Act can apply extraterritorially to providers from outside the EU if they have users within the EU.\\nProposed by the European Commission on 21 April 2021, it passed the European Parliament on 13 March 2024, and was unanimously approved by the EU Council on 21 May 2024. The draft Act was revised to address the rise in popularity of generative artificial intelligence systems, such as ChatGPT, whose general-purpose capabilities did not fit the main framework.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Artificial_Intelligence_Act'}, page_content='The Artificial Intelligence Act (AI Act) is a European Union regulation concerning artificial intelligence (AI). It establishes a common regulatory and legal framework for AI within the European Union (EU). It came into force on 1 August 2024, with provisions that shall come into operation gradually over the following 6 to 36 months.\\nIt covers all types of AI across a broad range of sectors, with exceptions for AI systems used solely for military, national security, research and non-professional purposes. As a piece of product regulation, it does not confer rights on individuals, but regulates the providers of AI systems and entities using AI in a professional context.\\nThe Act classifies non-exempt AI applications by their risk of causing harm. There are four levels – unacceptable, high, limited, minimal – plus an additional category for general-purpose AI. \\n\\nApplications with unacceptable risks are banned.\\nHigh-risk applications must comply with security, transparency and quality obligations, and undergo conformity assessments.\\nLimited-risk applications only have transparency obligations.\\nMinimal-risk applications are not regulated.\\nFor general-purpose AI, transparency requirements are imposed, with reduced requirements for open source models, and additional evaluations for high-capability models.\\nThe Act also creates a European Artificial Intelligence Board to promote national cooperation and ensure compliance with the regulation. Like the EU\\'s General Data Protection Regulation, the Act can apply extraterritorially to providers from outside the EU if they have users within the EU.\\nProposed by the European Commission on 21 April 2021, it passed the European Parliament on 13 March 2024, and was unanimously approved by the EU Council on 21 May 2024. The draft Act was revised to address the rise in popularity of generative artificial intelligence systems, such as ChatGPT, whose general-purpose capabilities did not fit the main framework.\\n\\n\\n== Provisions ==\\n\\n\\n=== Risk categories ===\\nThere are different risk categories depending on the type of application, with a specific category dedicated to general-purpose generative AI:\\n\\nUnacceptable risk – AI applications in this category are banned, except for specific exemptions. When no exemption applies, this includes AI applications that manipulate human behaviour, those that use real-time remote biometric identification (such as facial recognition) in public spaces, and those used for social scoring (ranking individuals based on their personal characteristics, socio-economic status, or behaviour).\\nHigh-risk – AI applications that are expected to pose significant threats to health, safety, or the fundamental rights of persons. Notably, AI systems used in health, education, recruitment, critical infrastructure management, law enforcement or justice. They are subject to quality, transparency, human oversight and safety obligations, and in some cases require a \"Fundamental Rights Impact Assessment\" before deployment. They must be evaluated both before they are placed on the market and throughout their life cycle. The list of high-risk applications can be expanded over time, without the need to modify the AI Act itself.\\nGeneral-purpose AI – Added in 2023, this category includes in particular foundation models like ChatGPT. Unless the weights and model architecture are released under free and open source licence, in which case only a training data summary and a copyright compliance policy are required, they are subject to transparency requirements. High-impact general-purpose AI systems including free and open source ones which could pose systemic risks (notably those trained using a computational capability exceeding 1025 FLOPS) must also undergo a thorough evaluation process.\\nLimited risk – AI systems in this category have transparency obligations, ensuring users are informed that they are interacting with an AI system and allowing them to make informed choices. This category includes, for'),\n",
       " Document(metadata={'title': 'List of artificial intelligence films', 'summary': 'This article contains a chronological list of notable films which included artificial intelligence either as a protagonist or as an essential part of the film.', 'source': 'https://en.wikipedia.org/wiki/List_of_artificial_intelligence_films'}, page_content='This article contains a chronological list of notable films which included artificial intelligence either as a protagonist or as an essential part of the film.\\n\\n\\n== 1920s-1960s ==\\n\\n\\n== 1970s ==\\n\\n\\n== 1980s ==\\n\\n\\n== 1990s ==\\n\\n\\n== 2000s ==\\n\\n\\n== 2010s ==\\n\\n\\n== 2020s ==\\n\\n\\n== See also ==\\nArtificial intelligence in fiction\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Glossary of artificial intelligence', 'summary': 'This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.', 'source': 'https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence'}, page_content='This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.\\n\\n\\n== A ==\\n\\nA* search\\nPronounced \"A-star\".\\nA graph traversal and pathfinding algorithm which is used in many fields of computer science due to its completeness, optimality, and optimal efficiency.\\n\\nabductive logic programming (ALP)\\nA high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates.\\n\\nabductive reasoning\\nAlso abduction.\\nA form of logical inference which starts with an observation or set of observations then seeks to find the simplest and most likely explanation. This process, unlike deductive reasoning, yields a plausible conclusion but does not positively verify it. abductive inference, or retroduction\\n\\nablation\\nThe removal of a component of an AI system. An ablation study aims to determine the contribution of a component to an AI system by removing the component, and then analyzing the resultant performance of the system.\\n\\nabstract data type\\nA mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations.\\n\\nabstraction\\nThe process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest\\n\\naccelerating change\\nA perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change.\\n\\naction language\\nA language for specifying state transition systems, and is commonly used to create formal models of the effects of actions on the world. Action languages are commonly used in the artificial intelligence and robotics domains, where they describe how actions affect the states of systems over time, and may be used for automated planning.\\n\\naction model learning\\nAn area of machine learning concerned with creation and modification of software agent\\'s knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners.\\n\\naction selection\\nA way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment.\\n\\nactivation function\\nIn artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs.\\n\\nadaptive algorithm\\nAn algorithm that changes its behavior at the time it is run, based on a priori defined reward mechanism or criterion.\\n\\nadaptive neuro fuzzy inference system (ANFIS)\\nAlso adaptive network-based fuzzy inference system.\\nA kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IF–THEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the'),\n",
       " Document(metadata={'title': 'Artificial intelligence arms race', 'summary': 'A military artificial intelligence arms race is an arms race between two or more states to develop and deploy lethal autonomous weapons systems (LAWS). Since the mid-2010s, many analysts have noted the emergence of such an arms race between superpowers for better military AI, driven by increasing geopolitical and military tensions. \\nAn AI arms race is sometimes placed in the context of an AI Cold War between the United States and China.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race'}, page_content='A military artificial intelligence arms race is an arms race between two or more states to develop and deploy lethal autonomous weapons systems (LAWS). Since the mid-2010s, many analysts have noted the emergence of such an arms race between superpowers for better military AI, driven by increasing geopolitical and military tensions. \\nAn AI arms race is sometimes placed in the context of an AI Cold War between the United States and China.\\n\\n\\n== Terminology ==\\nLethal autonomous weapons systems use artificial intelligence to identify and kill human targets without human intervention. LAWS have colloquially been called \"slaughterbots\" or \"killer robots\". Broadly, any competition for superior AI is sometimes framed as an \"arms race\". Advantages in military AI overlap with advantages in other sectors, as countries pursue both economic and military advantages.\\n\\n\\n== History ==\\nIn 2014, AI specialist Steve Omohundro warned that \"An autonomous weapons arms race is already taking place\". According to Siemens, worldwide military spending on robotics was US$5.1 billion in 2010 and US$7.5 billion in 2015.\\nChina became a top player in artificial intelligence research in the 2010s. According to the Financial Times, in 2016, for the first time, China published more AI papers than the entire European Union. When restricted to number of AI papers in the top 5% of cited papers, China overtook the United States in 2016 but lagged behind the European Union. 23% of the researchers presenting at the 2017 American Association for the Advancement of Artificial Intelligence (AAAI) conference were Chinese. Eric Schmidt, the former chairman of Alphabet, has predicted China will be the leading country in AI by 2025.\\n\\n\\n== Risks ==\\nOne risk concerns the AI race itself, whether or not the race is won by any one group. There are strong incentives for development teams to cut corners with regard to the safety of the system, increasing the risk of critical failures and unintended consequences. This is in part due to the perceived advantage of being the first to develop advanced AI technology. One team appearing to be on the brink of a breakthrough can encourage other teams to take shortcuts, ignore precautions and deploy a system that is less ready. Some argue that using \"race\" terminology at all in this context can exacerbate this effect.\\nAnother potential danger of an AI arms race is the possibility of losing control of the AI systems; the risk is compounded in the case of a race to artificial general intelligence, which may present an existential risk. In 2023, a United States Air Force official reportedly said that during a computer test, a simulated AI drone killed the human character operating it. The USAF later said the official had misspoken and that it never conducted such simulations.\\nA third risk of an AI arms race is whether or not the race is actually won by one group. The concern is regarding the consolidation of power and technological advantage in the hands of one group. A US government report argued that \"AI-enabled capabilities could be used to threaten  critical infrastructure, amplify disinformation campaigns, and wage war\":1, and that \"global stability and nuclear deterrence could be undermined\".:11\\n\\n\\n== Stances toward military artificial intelligence ==\\n\\n\\n=== United States ===\\n\\nIn 2014, former Secretary of Defense Chuck Hagel posited the \"Third Offset Strategy\" that rapid advances in artificial intelligence will define the next generation of warfare. According to data science and analytics firm Govini, the U.S. Department of Defense (DoD) increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2016. However, the civilian NSF budget for AI saw no increase in 2017. Japan Times reported in 2018 that the United States private investment is around $70 billion per year. The November 2019 \\'Interim Report\\' of the United States\\' National Security Commission on Artificial Intelligence c'),\n",
       " Document(metadata={'title': 'Artificial intelligence in education', 'summary': 'Artificial intelligence could be defined as \"systems which display intelligent behaviour by analysing their environment and taking actions – with some degree of autonomy – to achieve specific goals\". These systems might be software-based or embedded in hardware. The so called intelligence would either be displayed by following a rule-based or machine learning algorithm. Artificial intelligence in education (AIEd) is another vague term, and an interdisciplinary collection of fields which are bundled together, inter alia anthropomorphism, generative artificial intelligence, data-driven decision-making, AI ethics, classroom surveillance, data-privacy and AI literacy. An educator might learn to use these AI systems as tools and generate code, text or rich media or optimize their digital content production. Or a governmental body might see AI as an ideological project to normalize centralized power and decision making, while public schools and higher education contend with increasing privatization.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence_in_education'}, page_content='Artificial intelligence could be defined as \"systems which display intelligent behaviour by analysing their environment and taking actions – with some degree of autonomy – to achieve specific goals\". These systems might be software-based or embedded in hardware. The so called intelligence would either be displayed by following a rule-based or machine learning algorithm. Artificial intelligence in education (AIEd) is another vague term, and an interdisciplinary collection of fields which are bundled together, inter alia anthropomorphism, generative artificial intelligence, data-driven decision-making, AI ethics, classroom surveillance, data-privacy and AI literacy. An educator might learn to use these AI systems as tools and generate code, text or rich media or optimize their digital content production. Or a governmental body might see AI as an ideological project to normalize centralized power and decision making, while public schools and higher education contend with increasing privatization.\\n\\n\\n== Overview ==\\nThere is no single lens with which to understand AI in education (AIEd). At least three dominant paradigms have been suggested. Firstly the transmission paradigm, where AIEd systems represent a conduit for personalizing learning. Statistically probable text could be read and interpreted by a students, and the impression of insight and reason may arise. Secondly, the coordination paradigm, where AIEd is the supporter of a cohort\\'s knowledge construction, and this mass is socialized into new systems of thought. Alternately there is the leadership model, where individuals take agency and make choices about their learning (with or without AI) AIEd could be viewed as the ultimate disruptor, replacing academics and their scholarly prestige, or an opportunity to consider together, what makes humans different from machines.\\n\\n\\n== Emerging perspectives ==\\nThis complex social, cultural, and material assemblage should be seen in its geo-political context. It is likely that AI systems will be shaped by different policy or economic imperatives which will influence the construction, legitimation and use of this assemblage in an education setting. Those who see AI as a conduit for knowledge are comfortable with the idea of machine\\'s reasoning or having hallucinations. While those who are sceptics, recognize the cultivated \"closed-off imaginative spaces\" that big tech has captured, they notice how big tech\\'s discourse limits critical thought and discussions about these computational systems. Resistors often take a principled response and refuse to accept the many metaphors of \"artificial intelligence\", used to disguise working practices that are exploitative and extractive.\\n\\n\\n== The AI in education community ==\\nThe AI in education community has grown rapidly in the global north. Currently, there is much hype from venture capital, big tech and convinced open educationalists. AI in education is a contested terrain. Some educationalists believe that AI will remove the obstacle of \"access to expertise\". Others claim that education will be revolutionised with machines and their ability to understand natural language. While others are exploring how LLM\\'s \"reasoning\" might be improved. While in the global south, others see the AI\\'s data processing and monitoring as a misguided attempt to address colonialism and inequality, that that has inadvertently re-enforced a neo-liberal approach to education.\\n\\n\\n== Algorithms effects on education ==\\nAI companies that focus on education, are currently preoccupied with generative artificial intelligence (GAI), although data science and data analytics is another popular educational theme. At present, there is little scientific consensus on what AI is or how to classify and sub-categorize AI This has not hampered the growth of AI in education systems, which are gathering data and then optimising models.\\nAI offers scholars and students automatic assessment and feedback, predictions, instant machine translations'),\n",
       " Document(metadata={'title': 'Philosophy of artificial intelligence', 'summary': 'The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology,  and free will. Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers. These factors contributed to the emergence of the philosophy of artificial intelligence.\\nThe philosophy of artificial intelligence attempts to answer such questions as follows:\\n\\nCan a machine act intelligently? Can it solve any problem that a person would solve by thinking?\\nAre human intelligence and machine intelligence the same? Is the human brain essentially a computer?\\nCan a machine have a mind, mental states, and consciousness in the same sense that a human being can? Can it feel how things are? (i.e. does it have qualia?)\\nQuestions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\\nImportant propositions in the philosophy of AI include some of the following:\\n\\nTuring\\'s \"polite convention\": If a machine behaves as intelligently as a human being, then it is as intelligent as a human being.\\nThe Dartmouth proposal: \"Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\"\\nAllen Newell and Herbert A. Simon\\'s physical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\\nJohn Searle\\'s strong AI hypothesis: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"\\nHobbes\\' mechanism: \"For \\'reason\\' ... is nothing but \\'reckoning,\\' that is adding and subtracting, of the consequences of general names agreed upon for the \\'marking\\' and \\'signifying\\' of our thoughts...\"', 'source': 'https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence'}, page_content='The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology,  and free will. Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers. These factors contributed to the emergence of the philosophy of artificial intelligence.\\nThe philosophy of artificial intelligence attempts to answer such questions as follows:\\n\\nCan a machine act intelligently? Can it solve any problem that a person would solve by thinking?\\nAre human intelligence and machine intelligence the same? Is the human brain essentially a computer?\\nCan a machine have a mind, mental states, and consciousness in the same sense that a human being can? Can it feel how things are? (i.e. does it have qualia?)\\nQuestions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\\nImportant propositions in the philosophy of AI include some of the following:\\n\\nTuring\\'s \"polite convention\": If a machine behaves as intelligently as a human being, then it is as intelligent as a human being.\\nThe Dartmouth proposal: \"Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\"\\nAllen Newell and Herbert A. Simon\\'s physical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\\nJohn Searle\\'s strong AI hypothesis: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"\\nHobbes\\' mechanism: \"For \\'reason\\' ... is nothing but \\'reckoning,\\' that is adding and subtracting, of the consequences of general names agreed upon for the \\'marking\\' and \\'signifying\\' of our thoughts...\"\\n\\n\\n== Can a machine display general intelligence? ==\\nIs it possible to create a machine that can solve all the problems humans solve using their intelligence? This question defines the scope of what machines could do in the future and guides the direction of AI research. It only concerns the behavior of machines and ignores the issues of interest to psychologists, cognitive scientists and philosophers, evoking the question: does it matter whether a machine is really thinking, as a person thinks, rather than just producing outcomes that appear to result from thinking?\\nThe basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the Dartmouth workshop of 1956:\\n\\n\"Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\"\\nArguments against the basic premise must show that building a working AI system is impossible because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for intelligent behavior and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a system is possible.\\nIt is also possible to sidestep the connection between the two parts of the above proposal. For instance, machine learning, beginning with Turing\\'s infamous child machine proposal, essentially achieves the desired feature of intelligence without a precise design-time description as to how it would exactly work. The account on robot tacit knowledge eliminates the need for a precise description altogether.\\nThe first step to answering the question i'),\n",
       " Document(metadata={'title': 'Weak artificial intelligence', 'summary': 'Weak artificial intelligence (weak AI) is artificial intelligence that implements a limited part of the mind, or, as Artificial Narrow Intelligence, is focused on one narrow task. \\nWeak AI is contrasted with strong AI, which can be interpreted in various ways: \\n\\nArtificial general intelligence (AGI): a machine with the ability to apply intelligence to any problem, rather than just one specific problem.\\nArtificial super intelligence (ASI): a machine with a vastly superior intelligence to the average human being.\\nArtificial consciousness: a machine that has consciousness, sentience and mind (John Searle uses \"strong AI\" in this sense).\\nNarrow AI can be classified as being \"limited to a single, narrowly defined task. Most modern AI systems would be classified in this category.\" Artificial general intelligence is conversely the opposite.', 'source': 'https://en.wikipedia.org/wiki/Weak_artificial_intelligence'}, page_content='Weak artificial intelligence (weak AI) is artificial intelligence that implements a limited part of the mind, or, as Artificial Narrow Intelligence, is focused on one narrow task. \\nWeak AI is contrasted with strong AI, which can be interpreted in various ways: \\n\\nArtificial general intelligence (AGI): a machine with the ability to apply intelligence to any problem, rather than just one specific problem.\\nArtificial super intelligence (ASI): a machine with a vastly superior intelligence to the average human being.\\nArtificial consciousness: a machine that has consciousness, sentience and mind (John Searle uses \"strong AI\" in this sense).\\nNarrow AI can be classified as being \"limited to a single, narrowly defined task. Most modern AI systems would be classified in this category.\" Artificial general intelligence is conversely the opposite.\\n\\n\\n== Applications and risks ==\\nSome examples of narrow AI are AlphaGo, self-driving cars, robot systems used in the medical field, and diagnostic doctors. Narrow AI systems are sometimes dangerous if unreliable. And the behavior that it follows can become inconsistent. It could be difficult for the AI to grasp complex patterns and get to a solution that works reliably in various environments. This \"brittleness\" can cause it to fail in unpredictable ways.\\nNarrow AI failures can sometimes have significant consequences. It could for example cause disruptions in the electric grid, damage nuclear power plants, cause global economic problems, and misdirect autonomous vehicles. Medicines could be incorrectly sorted and distributed. Also, medical diagnoses can ultimately have serious and sometimes deadly consequences if the AI is faulty or biased.\\nSimple AI programs have already worked their way into our society unnoticed. Autocorrection for typing, speech recognition for speech-to-text programs, and vast expansions in the data science fields are examples. As much as narrow and relatively general AI is slowly starting to help out societies, they are also starting to hurt them as well. AI had already unfairly put people in jail, discriminated against women in the workplace for hiring, taught some problematic ideas to millions, and even killed people with automatic cars. AI might be a powerful tool that can be used for improving lives, but it could also be a dangerous technology with the potential for misuse.\\nDespite being \"narrow\" AI, recommender systems are efficient at predicting user reactions based their posts, patterns, or trends. For instance, TikTok\\'s \"For You\" algorithm can determine user\\'s interests or preferences in less than an hour. Some other social media AI systems are used to detect bots that may be involved in biased propaganda or other potentially malicious activities.\\n\\n\\n== Weak AI versus strong AI ==\\nJohn Searle contests the possibility of strong AI (by which he means conscious AI). He further believes that the Turing test (created by Alan Turing and originally called the \"imitation game\", used to assess whether a machine can converse indistinguishably from a human) is not accurate or appropriate for testing whether an AI is \"strong\".\\nScholars such as Antonio Lieto have argued that the current research on both AI and cognitive modelling are perfectly aligned with the weak-AI hypothesis (that should not be confused with the \"general\" vs \"narrow\" AI distinction) and that the popular assumption that cognitively inspired AI systems espouse the strong AI hypothesis is ill-posed and problematic since \"artificial models of brain and mind can be used to understand mental phenomena without pretending that that they are the real phenomena that they are modelling\" (as, on the other hand, implied by the strong AI assumption).\\n\\n\\n== See also ==\\nA.I. Rising – 2018 film directed by Lazar Bodroža\\nArtificial intelligence – Intelligence of machines\\nArtificial general intelligence – Type of AI with wide-ranging abilities\\nDeep learning – Branch of machine learning\\nExpert system – Computer system emulating the'),\n",
       " Document(metadata={'title': 'Artificial intelligence in fiction', 'summary': \"Artificial intelligence is a recurrent theme in science fiction, whether utopian, emphasising the potential benefits, or dystopian, emphasising the dangers.\\nThe notion of machines with human-like intelligence dates back at least to Samuel Butler's 1872 novel Erewhon. Since then, many science fiction stories have presented different effects of creating such intelligence, often involving rebellions by robots. Among the best known of these are Stanley Kubrick's 1968 2001: A Space Odyssey with its murderous onboard computer HAL 9000, contrasting with the more benign R2-D2 in George Lucas's 1977 Star Wars and the eponymous robot in Pixar's 2008 WALL-E.\\nScientists and engineers have noted the implausibility of many science fiction scenarios, but have mentioned fictional robots many times in artificial intelligence research articles, most often in a utopian context.\", 'source': 'https://en.wikipedia.org/wiki/Artificial_intelligence_in_fiction'}, page_content='Artificial intelligence is a recurrent theme in science fiction, whether utopian, emphasising the potential benefits, or dystopian, emphasising the dangers.\\nThe notion of machines with human-like intelligence dates back at least to Samuel Butler\\'s 1872 novel Erewhon. Since then, many science fiction stories have presented different effects of creating such intelligence, often involving rebellions by robots. Among the best known of these are Stanley Kubrick\\'s 1968 2001: A Space Odyssey with its murderous onboard computer HAL 9000, contrasting with the more benign R2-D2 in George Lucas\\'s 1977 Star Wars and the eponymous robot in Pixar\\'s 2008 WALL-E.\\nScientists and engineers have noted the implausibility of many science fiction scenarios, but have mentioned fictional robots many times in artificial intelligence research articles, most often in a utopian context.\\n\\n\\n== Background ==\\n\\nThe notion of advanced robots with human-like intelligence dates back at least to Samuel Butler\\'s 1872 novel Erewhon. This drew on an earlier (1863) article of his, Darwin among the Machines, where he raised the question of the evolution of consciousness among self-replicating machines that might supplant humans as the dominant species. Similar ideas were also discussed by others around the same time as Butler, including George Eliot in a chapter of her final published work Impressions of Theophrastus Such (1879). The creature in Mary Shelley\\'s 1818 Frankenstein has also been considered an artificial being, for instance by the science fiction author Brian Aldiss. Beings with at least some appearance of intelligence were imagined, too, in classical antiquity.\\n\\n\\n== Utopian and dystopian visions ==\\nArtificial intelligence is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals.\\nIt is a recurrent theme in science fiction; scholars have divided it into utopian, emphasising the potential benefits, and dystopian, emphasising the dangers.\\n\\n\\n=== Utopian ===\\n\\nOptimistic visions of the future of artificial intelligence are possible in science fiction. Benign AI characters include Robbie the Robot, first seen in Forbidden Planet on 1956; Data in Star Trek: The Next Generation from 1987 to 1994; and Pixar\\'s WALL-E in 2008. Iain Banks\\'s Culture series of novels portrays a utopian, post-scarcity space society of humanoids, aliens, and advanced beings with artificial intelligence living in socialist habitats across the Milky Way. Researchers at the University of Cambridge have identified four major themes in utopian scenarios featuring AI: immortality, or indefinite lifespans; ease, or freedom from the need to work; gratification, or pleasure and entertainment provided by machines; and dominance, the power to protect oneself or rule over others.\\nAlexander Wiegel contrasts the role of AI in 2001: A Space Odyssey and in Duncan Jones\\'s 2009 film Moon. Whereas in 1968, Wiegel argues, the public felt \"technology paranoia\" and the AI computer HAL was portrayed as a \"cold-hearted killer\", by 2009 the public were far more familiar with AI, and the film\\'s GERTY is \"the quiet savior\" who enables the protagonists to succeed, and who sacrifices itself for their safety.\\n\\n\\n=== Dystopian ===\\n\\nThe researcher Duncan Lucas writes (in 2002) that humans are worried about the technology they are constructing, and that as machines started to approach intellect and thought, that concern becomes acute. He calls the early 20th century dystopian view of AI in fiction the \"animated automaton\", naming as examples the 1931 film Frankenstein, the 1927 Metropolis, and the 1920 play R.U.R. A later 20th century approach he names \"heuristic hardware\", giving as instances 2001 a Space Odyssey, Do Androids Dream of Electric Sheep?, The Hitchhiker\\'s Guide to the Galaxy, and I, Robot. Lucas considers also the films that illustrate the effect of the personal computer on science fiction from 1980 onwards with the blurring of the boundary betwee')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "wikidocument = WikipediaLoader(query=\"Artificial Intelligence\", lang=\"en\").load()\n",
    "wikidocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikidocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
