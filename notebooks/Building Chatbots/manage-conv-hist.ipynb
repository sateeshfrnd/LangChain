{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. \n",
    "\n",
    "If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using trim_messages is a smart way to control the length of chat history that gets passed to the model â€” especially when your chat becomes long and might exceed token limits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GROQ key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000026524F92C20>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000026524FC40D0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model =ChatGroq(model=\"gemma2-9b-it\", groq_api_key = groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,              # Max tokens to allow in trimmed message list\n",
    "    strategy=\"last\",            # Start trimming from the oldest\n",
    "    token_counter=model,        # Uses the model to count tokens\n",
    "    include_system=True,        # Include the system prompt in the trimmed set\n",
    "    allow_partial=False,        # Don't allow partial messages if limit is reached\n",
    "    start_on=\"human\"            # Start trimming from the first human message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage,AIMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm sati\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmed = trimmer.invoke(messages)\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will start from the last human message, moving backward until the total token count reaches <= 45.\n",
    "\n",
    "It'll keep whole messages only (because allow_partial=False).\n",
    "\n",
    "It will include the SystemMessage since include_system=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token counting helper\n",
    "def print_token_count(messages, model):\n",
    "    print(\"Token Count Per Message:\")\n",
    "    for msg in messages:\n",
    "        tokens = model.get_num_tokens(msg.content)\n",
    "        print(f\"{msg.type:>6}: {tokens:>3} tokens | {msg.content}\")\n",
    "    total = sum(model.get_num_tokens(msg.content) for msg in messages)\n",
    "    print(f\"\\nTotal tokens: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count Per Message:\n",
      "system:   5 tokens | you're a good assistant\n",
      " human:   6 tokens | hi! I'm sati\n",
      "    ai:   2 tokens | hi!\n",
      " human:   5 tokens | I like vanilla ice cream\n",
      "    ai:   1 tokens | nice\n",
      " human:   5 tokens | whats 2 + 2\n",
      "    ai:   1 tokens | 4\n",
      " human:   1 tokens | thanks\n",
      "    ai:   3 tokens | no problem!\n",
      " human:   3 tokens | having fun?\n",
      "    ai:   2 tokens | yes!\n",
      "\n",
      "Total tokens: 34\n"
     ]
    }
   ],
   "source": [
    "print_token_count(messages, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count Per Message:\n",
      "system:   5 tokens | you're a good assistant\n",
      " human:   5 tokens | I like vanilla ice cream\n",
      "    ai:   1 tokens | nice\n",
      " human:   5 tokens | whats 2 + 2\n",
      "    ai:   1 tokens | 4\n",
      " human:   1 tokens | thanks\n",
      "    ai:   3 tokens | no problem!\n",
      " human:   3 tokens | having fun?\n",
      "    ai:   2 tokens | yes!\n",
      "\n",
      "Total tokens: 26\n"
     ]
    }
   ],
   "source": [
    "print_token_count(trimmed, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pass trimmed the messages to chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# A passthrough step that trims the messages\n",
    "preprocessor = RunnablePassthrough.assign(\n",
    "    messages=lambda x: trimmer.invoke(x[\"messages\"])\n",
    ")\n",
    "\n",
    "# Prompt with messages placeholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer in {language}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "\n",
    "# Final chain\n",
    "chat_chain = preprocessor | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know what your favorite ice cream is! I'm just a language model, I can't remember past conversations or personal information about you.  \\n\\nTell me, what's your favorite flavor? ðŸ¦\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_chain.invoke({\n",
    "    \"language\": \"Telugu\",\n",
    "    \"messages\": messages  + [HumanMessage(content=\"What ice cream do i like\")] # untrimmed messages\n",
    "})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"what\\'s 2 + 2\".  ðŸ§®  \\n\\n\\n\\nLet me know if you want to try another one!\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the MEssage History\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory # to tracks messages in memory.\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# In-memory session store\n",
    "store={}\n",
    "\n",
    "# Get/Create chat history for a session\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory() # ensures each session ID gets its own message history.\n",
    "    return store[session_id]\n",
    "\n",
    "# wraps the model with message history, so every invocation keeps context based on the session.\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chat_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and don't know your name.  \\n\\nWhat's your name? ðŸ˜Š\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
