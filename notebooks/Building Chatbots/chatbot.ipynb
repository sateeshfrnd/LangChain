{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GROQ key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002378D5B3190>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002378D5E0640>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model =ChatGroq(model=\"gemma2-9b-it\", groq_api_key = groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Satish, it's nice to meet you!\\n\\nI'm glad to hear you're a Data Engineer. That's a really interesting field. \\n\\nWhat kind of projects are you working on these days? \\n\\nI'm always eager to learn more about what people are doing in the data world.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 24, 'total_tokens': 94, 'completion_time': 0.127272727, 'prompt_time': 0.002117557, 'queue_time': 0.238227721, 'total_time': 0.129390284}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-cb4f145d-5e80-4648-90e0-eff8eb32bd56-0', usage_metadata={'input_tokens': 24, 'output_tokens': 70, 'total_tokens': 94})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Satish and I am working as a Data Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You said your name is Satish, and you work as a Data Engineer. üòä  \\n\\nIs there anything else you'd like to tell me about your work?  I'm curious to hear more!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 115, 'total_tokens': 161, 'completion_time': 0.083636364, 'prompt_time': 0.005311202, 'queue_time': 0.23443555700000002, 'total_time': 0.088947566}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-12326a06-e57a-41a4-b1d4-e0fc68c9f972-0', usage_metadata={'input_tokens': 115, 'output_tokens': 46, 'total_tokens': 161})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Satish and  I am working as a Data Engineer\"),\n",
    "        AIMessage(content=\"Hi Satish, it's nice to meet you!\\n\\nI'm glad to hear you're a Data Engineer. That's a really interesting field. \\n\\nWhat kind of projects are you working on these days? \\n\\nI'm always eager to learn more about what people are doing in the data world.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 24, 'total_tokens': 94, 'completion_time': 0.127272727, 'prompt_time': 0.002117557, 'queue_time': 0.238227721, 'total_time': 0.129390284}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-cb4f145d-5e80-4648-90e0-eff8eb32bd56-0', usage_metadata={'input_tokens': 24, 'output_tokens': 70, 'total_tokens': 94}),\n",
    "        HumanMessage(content=\"Hey what's my name and what do I do ?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History\n",
    "Message history in a chatbot is the record of previous user and bot messages. It helps the chatbot maintain context, understand follow-up questions, and respond more naturally in ongoing conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory # to tracks messages in memory.\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# In-memory session store\n",
    "store={}\n",
    "\n",
    "# Get/Create chat history for a session\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory() # ensures each session ID gets its own message history.\n",
    "    return store[session_id]\n",
    "\n",
    "# wraps the model with message history, so every invocation keeps context based on the session.\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"chat1\"\n",
    "config = {\"configurable\": {\"session_id\": session_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First user message\n",
    "response1 = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is Satish and I am a Data Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Satish, it's nice to meet you! üëã\\n\\nBeing a Data Engineer is a really interesting field. What kind of projects are you working on these days?  Anything exciting? üòÑ  \\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Satish!  üòä  \\n\\nI remember that from our introduction.  Is there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 80, 'total_tokens': 111, 'completion_time': 0.056363636, 'prompt_time': 0.004176253, 'queue_time': 0.235540795, 'total_time': 0.060539889}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-58f8b75c-a515-44d7-8ec6-77c691393d6c-0', usage_metadata={'input_tokens': 80, 'output_tokens': 31, 'total_tokens': 111})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second user message (uses previous context automatically)\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Of course, Satish! I can definitely suggest some data engineering projects. \\n\\nTo give you the best recommendations, tell me:\\n\\n1. **What's your current skill level?** (Beginner, intermediate, advanced)\\n2. **What are you most interested in?** (e.g., real-time data processing, machine learning pipelines, data warehousing, etc.)\\n3. **Do you prefer working with specific tools or technologies?** (e.g., Spark, Kafka, Airflow, Snowflake, etc.)\\n4. **Do you have a specific domain in mind?** (e.g., finance, healthcare, e-commerce, etc.)\\n\\nOnce I have this information, I can suggest projects that are both challenging and rewarding for you. \\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 164, 'prompt_tokens': 124, 'total_tokens': 288, 'completion_time': 0.298181818, 'prompt_time': 0.006743298, 'queue_time': 0.240107232, 'total_time': 0.304925116}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7d5bed37-bc79-4120-9821-c0e237b62f6c-0', usage_metadata={'input_tokens': 124, 'output_tokens': 164, 'total_tokens': 288})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second user message (uses previous context automatically)\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Can you suggest good projects\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You‚Äôll see the bot remembers the context (e.g., your name and role) in the second message and replies accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New session\n",
    "new_session_id = \"chat2\"\n",
    "new_config = {\"configurable\": {\"session_id\": new_session_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no access to your personal information, including your name. If you'd like to tell me your name, I'd be happy to know!\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First message in new session\n",
    "response_new = with_message_history.invoke(\n",
    "   [HumanMessage(content=\"What's my name?\")],\n",
    "    config=new_config\n",
    ")\n",
    "response_new.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Satish!  üòä  \\n\\nI remember that from our earlier conversation.  \\n\\nIs there anything else I can help you with today, Satish?\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "   [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Session chat1: Bot has prior context like your name and role.\n",
    "- Session chat2: Bot doesn‚Äôt know anything yet ‚Äî clean slate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's great to meet you, Kumar! Learning about Generative AI is exciting. \\n\\nWhat specifically are you interested in learning about? I can try my best to answer your questions or provide you with helpful resources. \\n\\nFor example, are you curious about:\\n\\n* **How Gen AI works?**\\n* **Different types of Gen AI models (like me!)?**\\n* **Applications of Gen AI in various fields?**\\n* **The ethical considerations surrounding Gen AI?**\\n\\n\\nLet me know what sparks your interest! üòä\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_new = with_message_history.invoke(\n",
    "   [HumanMessage(content=\"My name is Kumar and learning Gen AI\")],\n",
    "    config=new_config\n",
    ")\n",
    "response_new.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your name is Kumar earlier!  üòä \\n\\nIs there anything else I can help you with, Kumar?  \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First message in new session\n",
    "response_new = with_message_history.invoke(\n",
    "   [HumanMessage(content=\"What's my name?\")],\n",
    "    config=new_config\n",
    ")\n",
    "response_new.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the question to the nest of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What MessagesPlaceholder Does**\n",
    "\n",
    "It acts as a slot where LangChain injects prior messages (from memory/history) automatically.\n",
    "\n",
    "This is perfect for open-ended chatbots where you want to maintain back-and-forth context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain with output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain=prompt|model|StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Satish! It's nice to meet you.  \\n\\nI'm happy to help with any questions you have.  \\n\\nWhat can I do for you today? üòä  \\n\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi! My name is Satish\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap chain with message history\n",
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a conversation\n",
    "config = {\"configurable\": {\"session_id\": \"chat03\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Satish.  \\n\\nIs there anything else I can help you with?  üòÑ \\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt with MessagesPlaceholder + language variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Always reply in {language}, no matter what the input language is. Your job is to answer all questions to the best of your ability.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Chain with message history\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",     # for the user input text\n",
    "    history_messages_key=\"messages\" # this injects prior messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat04\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞æ‡∞ï‡±Å ‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞§‡±Ü‡∞≤‡∞ø‡∞Ø‡∞¶‡±Å. ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞®‡∞æ‡∞ï‡±Å ‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞ó‡∞≤‡∞∞‡±Ç? üòä \\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1 = chat_chain.invoke(\n",
    "    {\"input\": \"What is my name?\", \"language\": \"TElugu\"},\n",
    "    config=config\n",
    ")\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞Æ‡∞∏‡±ç‡∞§‡±á ‡∞∂‡±ç‡∞∞‡±Ä ‡∞∏‡∞§‡±Ä‡∞∑‡±ç! ‡∞¨‡±Ü‡∞Ç‡∞ó‡∞≥‡±Ç‡∞∞‡±Å‡∞≤‡±ã ‡∞®‡∞ø‡∞µ‡∞æ‡∞∏‡∞Ç ‡∞â‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞®‡∞ø ‡∞§‡±Ü‡∞≤‡∞ø‡∞∏‡∞ø‡∞Ç‡∞¶‡∞ø. üòä \\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2 = chat_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"My name is Satish staying in Bangalore.\",\n",
    "        \"language\": \"Telugu\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞Æ‡±Ä ‡∞™‡±á‡∞∞‡±Å ‡∞∂‡±ç‡∞∞‡±Ä ‡∞∏‡∞§‡±Ä‡∞∑‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞¨‡±Ü‡∞Ç‡∞ó‡∞≥‡±Ç‡∞∞‡±Å‡∞≤‡±ã ‡∞®‡∞ø‡∞µ‡∞∏‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å. üòä  \\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chat_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What is my name and where do I stay?\",\n",
    "        \"language\": \"Telugu\"\n",
    "    },\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
