{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content=\"LangChain is an open-source framework designed to help developers build applications powered by large language models (LLMs).\\n\\nIt provides tools for loading, processing, and managing different types of data sources such as text files, PDFs, web pages, and databases.\\n\\nUsing LangChain's document loaders, we can efficiently fetch data from multiple sources and utilize it for various AI-based tasks.\\n\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"sample.txt\")\n",
    "doc = loader.load()\n",
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample.txt'}, page_content='LangChain is an open-source framework designed to help developers build applications powered by'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='by large language models (LLMs).'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='It provides tools for loading, processing, and managing different types of data sources such as'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='such as text files, PDFs, web pages, and databases.'),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content=\"Using LangChain's document loaders, we can efficiently fetch data from multiple sources and utilize\"),\n",
       " Document(metadata={'source': 'sample.txt'}, page_content='utilize it for various AI-based tasks.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=10)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satee\\AppData\\Local\\Temp\\ipykernel_52520\\1715363856.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model='gemma2:2b')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model='gemma2:2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x158e2804be0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by large language models (LLMs).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Langchain\"\n",
    "query_result = vectordb.similarity_search(query= query)\n",
    "query_result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Disk\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory='./chroma_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load vector db\n",
    "chroma_db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by large language models (LLMs).'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = chroma_db.similarity_search(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='dbf85943-5e3a-4a56-a0c1-6bf5ba2f7eef', metadata={'source': 'sample.txt'}, page_content='by large language models (LLMs).'),\n",
       "  8560.378163357245),\n",
       " (Document(id='34b65273-e607-4b0a-ae8d-1d28f51feaa4', metadata={'source': 'sample.txt'}, page_content='utilize it for various AI-based tasks.'),\n",
       "  9334.688244095338),\n",
       " (Document(id='80e030e6-e9fb-4b32-92f2-09cd971e94b2', metadata={'source': 'sample.txt'}, page_content=\"Using LangChain's document loaders, we can efficiently fetch data from multiple sources and utilize\"),\n",
       "  11855.716835248231),\n",
       " (Document(id='8f1ea4d7-6b73-4fe2-83cc-9c3009263fff', metadata={'source': 'sample.txt'}, page_content='such as text files, PDFs, web pages, and databases.'),\n",
       "  12009.64020022119)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_score = vectordb.similarity_search_with_score(query)\n",
    "docs_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a Retriever\n",
    "\n",
    "We can aslos convert the vectore store into a Retriever calss. This allows us to easily use it in other LangChain methods, wich largely work with retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by large language models (LLMs).'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "query_results = retriever.invoke(query)\n",
    "query_results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
