{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a6b1b1",
   "metadata": {},
   "source": [
    "# Buliding Agent with Tools Wikipedia, Arxiv, and RAG using LangChain and GROQ LLM\n",
    "\n",
    "In this notebook, we build a powerful multi-tool AI agent using LangChain and GROQ's fast and efficient LLaMA3 model. The agent can answer complex questions by combining reasoning with external knowledge sources and custom document retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36274333",
   "metadata": {},
   "source": [
    "### Create Tools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f0db6",
   "metadata": {},
   "source": [
    "#### Tool for Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3a373e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Create wrapper for Wikipedia API\n",
    "wiki_api_wrapper = WikipediaAPIWrapper(\n",
    "    top_k_results=1,\n",
    "    doc_content_chars_max=250\n",
    ")\n",
    "# Create function to query Wikipedia\n",
    "wiki_func = WikipediaQueryRun(api_wrapper=wiki_api_wrapper)\n",
    "wiki_func.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a715efe7",
   "metadata": {},
   "source": [
    "#### Tool for Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd9b9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "\n",
    "# Create wrapper for Arxiv API\n",
    "arxiv_api_wrapper = ArxivAPIWrapper(\n",
    "    top_k_results=1,\n",
    "    doc_content_chars_max=250\n",
    ")\n",
    "# Create function to query Arxiv\n",
    "arxiv_func = ArxivQueryRun(api_wrapper=arxiv_api_wrapper)\n",
    "print(arxiv_func.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5ffec",
   "metadata": {},
   "source": [
    "#### Custom Tool (RAG Tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363efcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "\n",
      "{'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# Create a web loader for a specific URL\n",
    "web_loader = WebBaseLoader(\n",
    "    web_path=\"https://docs.smith.langchain.com/\"\n",
    ")\n",
    "docs = web_loader.load()\n",
    "# Print the loaded documents\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:50])  \n",
    "    print(doc.metadata)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf486598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter to split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "# Split the loaded documents into smaller chunks\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Print the first chunk of the first document\n",
    "print(split_docs[0].page_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9392e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\SATISH-KUMAR\\GitRepos\\LangChain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BCA2A31D50>, search_kwargs={'k': 2})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create an embeddings model using Hugging Face\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create a vector store using FAISS with the embeddings model\n",
    "vector_store = FAISS.from_documents(\n",
    "    split_docs,\n",
    "    embeddings_model\n",
    ")\n",
    "# Print the number of documents in the vector store\n",
    "print(len(vector_store.index_to_docstore_id))\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "# print the retriever's search type and number of results to return\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcf3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='LangSmithSearch', description='A tool to retrieve from the LangSmith knowledge base.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001BCCEF9E3B0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BCA2A31D50>, search_kwargs={'k': 2}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001BCCF310820>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BCA2A31D50>, search_kwargs={'k': 2}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a retriever tool using the retriever\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Create a retriever tool using the retriever\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"LangSmithSearch\",\n",
    "    description=\"A tool to retrieve from the LangSmith knowledge base.\"\n",
    ")\n",
    "\n",
    "# Print the retriever tool's name and description\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7082e",
   "metadata": {},
   "source": [
    "#### Create Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b4a015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n",
      "A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "<class 'langchain_community.tools.wikipedia.tool.WikipediaQueryInput'>\n",
      "\n",
      "arxiv\n",
      "A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "<class 'langchain_community.tools.arxiv.tool.ArxivInput'>\n",
      "\n",
      "LangSmithSearch\n",
      "A tool to retrieve from the LangSmith knowledge base.\n",
      "<class 'langchain_core.tools.retriever.RetrieverInput'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tools = [wiki_func, arxiv_func, retriever_tool]\n",
    "# Print the names of the tools\n",
    "for tool in tools:\n",
    "    print(tool.name)\n",
    "    print(tool.description)\n",
    "    print(tool.args_schema)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ada6be",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f31a279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001BCC1BC1F90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001BCC1BC3400>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Groq model using the Groq API\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model_name=\"Llama3-8b-8192\",\n",
    "    api_key=groq_api_key\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d497f",
   "metadata": {},
   "source": [
    "#### Prompt Template for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23160fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001BCCECB75B0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI agent that can search Wikipedia and Arxiv.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI agent that can search Wikipedia and Arxiv.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ca3c1",
   "metadata": {},
   "source": [
    "#### Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd64b22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: message_formatter(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001BCCECB75B0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI agent that can search Wikipedia and Arxiv.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001BCC1BC1F90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001BCC1BC3400>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'LangSmithSearch', 'description': 'A tool to retrieve from the LangSmith knowledge base.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| ToolsAgentOutputParser()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53f6c7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: message_formatter(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001BCCECB75B0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI agent that can search Wikipedia and Arxiv.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001BCC1BC1F90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001BCC1BC3400>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'LangSmithSearch', 'description': 'A tool to retrieve from the LangSmith knowledge base.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| ToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\SATISH-KUMAR\\\\GitRepos\\\\LangChain\\\\venv\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)), Tool(name='LangSmithSearch', description='A tool to retrieve from the LangSmith knowledge base.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001BCCEF9E3B0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BCA2A31D50>, search_kwargs={'k': 2}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001BCCF310820>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BCA2A31D50>, search_kwargs={'k': 2}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrap with executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42645401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'LangSmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Will Smith\n",
      "Summary: Willard Carroll  Smith II (born September 25, 1968) is an American actor, rapper, and film producer. He has received multiple accolades, including an Academy Award, a Golden Globe Award, a BAFTA Award, and four Grammy Awards\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'langsmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2020-10-09\n",
      "Title: Langsmith: An Interactive Academic Text Revision System\n",
      "Authors: Takumi Ito, Tatsuki Kuribayashi, Masatoshi Hidaka, Jun Suzuki, Kentaro Inui\n",
      "Summary: Despite the current diversity and inclusion initiatives in the academic\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that you've called the tool to retrieve information about LangSmith, and it yielded a research paper about the same topic.\n",
      "\n",
      "Based on the information provided, it appears that LangSmith is an interactive academic text revision system. It's likely a tool designed to help improve the quality and accuracy of academic papers.\n",
      "\n",
      "As you're interested in testing your LLMs, LangSmith might be a useful resource for you. You could try using it to generate text, revise text, or even use it as a dataset to train your models.\n",
      "\n",
      "Since you've already called the tools to gather information about LangSmith, I'll respond directly to your question. LangSmith seems like a fascinating tool, and I'm sure it has many features that could be useful for testing and training your LLMs. If you have any specific questions about how to use LangSmith or how to integrate it with your LLMs, feel free to ask!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is LangSmith? What are some of its features? How can I use it to test my LLMs?',\n",
       " 'output': \"It seems that you've called the tool to retrieve information about LangSmith, and it yielded a research paper about the same topic.\\n\\nBased on the information provided, it appears that LangSmith is an interactive academic text revision system. It's likely a tool designed to help improve the quality and accuracy of academic papers.\\n\\nAs you're interested in testing your LLMs, LangSmith might be a useful resource for you. You could try using it to generate text, revise text, or even use it as a dataset to train your models.\\n\\nSince you've already called the tools to gather information about LangSmith, I'll respond directly to your question. LangSmith seems like a fascinating tool, and I'm sure it has many features that could be useful for testing and training your LLMs. If you have any specific questions about how to use LangSmith or how to integrate it with your LLMs, feel free to ask!\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is LangSmith? What are some of its features? How can I use it to test my LLMs?\"\n",
    "    }\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9c662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
